{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fa019e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, log_loss, roc_auc_score\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2543d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, x_test, y_test):\n",
    "    \n",
    "    probs = model.predict_proba(x_test)\n",
    "    \n",
    "    predicted_probs = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if len(idx_arr) == 0:\n",
    "            predicted_probs.append(log_loss(y_true = [1,0], y_pred=[0,1])+1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            true_label_one_hot = np.zeros_like(probs[i])\n",
    "            true_label_one_hot[idx_arr] = 1\n",
    "            predicted_probs.append(log_loss(y_true = true_label_one_hot, y_pred = probs[i]))\n",
    "            \n",
    "    return np.array(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39876c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_loss(model, x_test, y_test):\n",
    "    \n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    predicted_probs = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if len(idx_arr) == 0:\n",
    "            predicted_probs.append(1.1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            \n",
    "            true_label_one_hot = np.zeros_like(probs[i])\n",
    "            true_label_one_hot[idx_arr] = 1\n",
    "            predicted_probs.append(1-probs[i][col_index])\n",
    "            \n",
    "    return np.array(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48605b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform_target(encoder, targets, unknown_value=-1):\n",
    "    classes = set(encoder.classes_)\n",
    "    transformed = []\n",
    "    for t in targets:\n",
    "        if t in classes:\n",
    "            transformed.append(encoder.transform([t])[0])\n",
    "        else:\n",
    "            transformed.append(unknown_value)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400367cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_loss(normal_loss_value, cross_entropy_loss_value):\n",
    "    normal_loss_dist = []\n",
    "    cross_loss_dist = []\n",
    "    for pos, prediction in  enumerate(normal_loss_value):\n",
    "        if prediction != 1:\n",
    "            cross_loss_dist.append(cross_entropy_loss_value[pos])\n",
    "            normal_loss_dist.append(prediction)\n",
    "\n",
    "    return normal_loss_dist, cross_loss_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68922aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cls_result(classification_result):\n",
    "    \n",
    "    for i in classification_result.keys():\n",
    "        print(i, classification_result[i].keys())\n",
    "\n",
    "        if '1' not in classification_result[i].keys():\n",
    "            classification_result[i]['1'] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0.0}\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986b12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Read and Process the Data\n",
    "# ----------------------------\n",
    "dataset = '0.099_sample.csv'\n",
    "df = pd.read_csv(\"../data/%s\" % (dataset))\n",
    "df = df.sort_values(by='Timestamp')\n",
    "# Process the 'noise' column:\n",
    "# - If NaN, assume Normal (0).\n",
    "# - Otherwise, treat True/1/'True' as anomaly (1); everything else as Normal (0).\n",
    "df['noise'] = df['noise'].fillna(0).apply(lambda x: 1 if (x == True or x == 1 or x == 'True' or x=='true') else 0)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(dataset)\n",
    "# Calculate the cutoff time (e.g., the median of all timestamps)\n",
    "cutoff_time = df['Timestamp'].median()\n",
    "\n",
    "anomaly_f1_list = []\n",
    "anomaly_support_list = []\n",
    "prefix_range = range(2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240998b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training window size: 0.8\n",
      "Total cases with at least 2 events: 500\n",
      "Encoded feature shape: (500, 17)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 1: threshold=0.496 Cross entropy diff=0.449\n",
      "Kneedle Normal loss=0.9976414451579514 Drop diff normal loss=0.9947933838517131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases with at least 3 events: 500\n",
      "Encoded feature shape: (500, 34)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 3: threshold=0.373 Cross entropy diff=0.373\n",
      "Kneedle Normal loss=0.9873638659589311 Drop diff normal loss=0.9873638659589311\n",
      "Total cases with at least 4 events: 500\n",
      "Encoded feature shape: (500, 52)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 1: threshold=0.428 Cross entropy diff=0.377\n",
      "Kneedle Normal loss=0.995 Drop diff normal loss=0.9198333333333333\n",
      "Total cases with at least 5 events: 500\n",
      "Encoded feature shape: (500, 70)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 1: threshold=0.442 Cross entropy diff=0.376\n",
      "Kneedle Normal loss=0.970375935243061 Drop diff normal loss=0.714\n",
      "Total cases with at least 6 events: 500\n",
      "Encoded feature shape: (500, 88)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 1: threshold=0.351 Cross entropy diff=0.303\n",
      "Kneedle Normal loss=0.9917098125131069 Drop diff normal loss=0.8009478660235245\n",
      "Total cases with at least 7 events: 500\n",
      "Encoded feature shape: (500, 105)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 1: threshold=0.293 Cross entropy diff=0.169\n",
      "Kneedle Normal loss=0.9983148110496638 Drop diff normal loss=0.6565374699628226\n",
      "Total cases with at least 8 events: 500\n",
      "Encoded feature shape: (500, 123)\n",
      "Training cases: 400 Test cases: 100\n",
      "Kneedle knee at position 26: threshold=0.084 Cross entropy diff=0.269\n",
      "Kneedle Normal loss=0.6699999999999999 Drop diff normal loss=0.9793835295714157\n",
      "Total cases with at least 9 events: 446\n",
      "Encoded feature shape: (446, 141)\n",
      "Training cases: 356 Test cases: 90\n",
      "Kneedle knee at position 11: threshold=0.128 Cross entropy diff=0.128\n",
      "Kneedle Normal loss=0.8384644246399755 Drop diff normal loss=0.8384644246399755\n",
      "Total cases with at least 10 events: 402\n",
      "Encoded feature shape: (402, 158)\n",
      "Training cases: 321 Test cases: 81\n",
      "Kneedle knee at position 21: threshold=0.096 Cross entropy diff=1\n",
      "Kneedle Normal loss=0.6799999999999999 Drop diff normal loss=0.8147834076842697\n",
      "Total cases with at least 11 events: 373\n",
      "Encoded feature shape: (373, 171)\n",
      "Training cases: 298 Test cases: 75\n",
      "Kneedle knee at position 18: threshold=0.056 Cross entropy diff=0.067\n",
      "Kneedle Normal loss=0.79 Drop diff normal loss=0.85\n",
      "Total cases with at least 12 events: 318\n",
      "Encoded feature shape: (318, 187)\n",
      "Training cases: 254 Test cases: 64\n",
      "Kneedle knee at position 22: threshold=0.062 Cross entropy diff=0.114\n",
      "Kneedle Normal loss=0.61 Drop diff normal loss=0.95\n",
      "Total cases with at least 13 events: 241\n",
      "Encoded feature shape: (241, 199)\n",
      "Training cases: 192 Test cases: 49\n",
      "Kneedle knee at position 1: threshold=0.125 Cross entropy diff=1\n",
      "Kneedle Normal loss=0.99 Drop diff normal loss=0.9163809523809524\n",
      "Total cases with at least 14 events: 201\n",
      "Encoded feature shape: (201, 203)\n",
      "Training cases: 160 Test cases: 41\n",
      "Kneedle knee at position 6: threshold=0.06 Cross entropy diff=0.095\n",
      "Kneedle Normal loss=0.9 Drop diff normal loss=0.97\n",
      "Total cases with at least 15 events: 171\n",
      "Encoded feature shape: (171, 206)\n",
      "Training cases: 136 Test cases: 35\n",
      "Kneedle knee at position 12: threshold=0.05 Cross entropy diff=0.058\n",
      "Kneedle Normal loss=0.73 Drop diff normal loss=0.9299999999999999\n",
      "2 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "3 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "4 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "5 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "6 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "7 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "8 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "9 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "10 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "11 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "12 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "13 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "14 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "15 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefix length</th>\n",
       "      <th>Normal precision</th>\n",
       "      <th>Normal recall</th>\n",
       "      <th>Normal f1-score</th>\n",
       "      <th>Normal support</th>\n",
       "      <th>Anomal precision</th>\n",
       "      <th>Anomal recall</th>\n",
       "      <th>Anomal f1-score</th>\n",
       "      <th>Anomal support</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Macro f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.938950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.994382</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.955791</td>\n",
       "      <td>0.955791</td>\n",
       "      <td>0.955791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.842391</td>\n",
       "      <td>0.584229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.842466</td>\n",
       "      <td>0.611632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.422161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.657759</td>\n",
       "      <td>0.778963</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.420202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.284444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prefix length  Normal precision  Normal recall  Normal f1-score  \\\n",
       "0               2          0.890000       1.000000         0.941799   \n",
       "1               3          0.958763       1.000000         0.978947   \n",
       "2               4          0.978022       0.978022         0.978022   \n",
       "3               5          0.978261       1.000000         0.989011   \n",
       "4               6          0.988764       1.000000         0.994350   \n",
       "5               7          0.988506       0.988506         0.988506   \n",
       "6               8          1.000000       0.684783         0.812903   \n",
       "7               9          1.000000       0.851852         0.920000   \n",
       "8              10          1.000000       0.684932         0.813008   \n",
       "9              11          1.000000       0.428571         0.600000   \n",
       "10             12          1.000000       0.440678         0.611765   \n",
       "11             13          0.965517       0.682927         0.800000   \n",
       "12             14          1.000000       0.447368         0.618182   \n",
       "13             15          1.000000       0.323529         0.488889   \n",
       "\n",
       "    Normal support  Anomal precision  Anomal recall  Anomal f1-score  \\\n",
       "0             89.0          0.000000       0.000000         0.000000   \n",
       "1             93.0          1.000000       0.428571         0.600000   \n",
       "2             91.0          0.777778       0.777778         0.777778   \n",
       "3             90.0          1.000000       0.800000         0.888889   \n",
       "4             88.0          1.000000       0.916667         0.956522   \n",
       "5             87.0          0.923077       0.923077         0.923077   \n",
       "6             92.0          0.216216       1.000000         0.355556   \n",
       "7             81.0          0.428571       1.000000         0.600000   \n",
       "8             73.0          0.258065       1.000000         0.410256   \n",
       "9             70.0          0.111111       1.000000         0.200000   \n",
       "10            59.0          0.131579       1.000000         0.232558   \n",
       "11            41.0          0.350000       0.875000         0.500000   \n",
       "12            38.0          0.125000       1.000000         0.222222   \n",
       "13            34.0          0.041667       1.000000         0.080000   \n",
       "\n",
       "    Anomal support  Macro precision  Macro recall  Macro f1-score  \n",
       "0             11.0         0.445000      0.500000        0.470899  \n",
       "1              7.0         0.979381      0.714286        0.789474  \n",
       "2              9.0         0.877900      0.877900        0.877900  \n",
       "3             10.0         0.989130      0.900000        0.938950  \n",
       "4             12.0         0.994382      0.958333        0.975436  \n",
       "5             13.0         0.955791      0.955791        0.955791  \n",
       "6              8.0         0.608108      0.842391        0.584229  \n",
       "7              9.0         0.714286      0.925926        0.760000  \n",
       "8              8.0         0.629032      0.842466        0.611632  \n",
       "9              5.0         0.555556      0.714286        0.400000  \n",
       "10             5.0         0.565789      0.720339        0.422161  \n",
       "11             8.0         0.657759      0.778963        0.650000  \n",
       "12             3.0         0.562500      0.723684        0.420202  \n",
       "13             1.0         0.520833      0.661765        0.284444  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = 0.8\n",
    "print('Training window size: %s' % (training_size))\n",
    "loss_prefix_dict =dict()\n",
    "classification_result = dict()\n",
    "anomaly_thr_method = 'diff'\n",
    "adaptive_thr_dict = dict()\n",
    "\n",
    "for prefix in prefix_range:    \n",
    "    # Extract per case:\n",
    "    # - The first (prefix-1) events (activities) as features.\n",
    "    # - The prefix-th event's activity as the target.\n",
    "    # - The prefix-th event's noise flag as the ground truth anomaly.\n",
    "    case_features = []\n",
    "    case_targets = []\n",
    "    ground_truth_anomaly = []\n",
    "\n",
    "    for case_id, group in df.groupby('ID'):\n",
    "        group = group.sort_index()  # assuming the order in the file is the event order\n",
    "        if len(group) >= prefix:\n",
    "            events = group['Activity'].values  # adjust 'Activity' if needed\n",
    "            features = events[:prefix-1]\n",
    "            target_activity = events[prefix-1]  # prefix-th event's activity\n",
    "            noise_flag = group['noise'].iloc[prefix-1]\n",
    "\n",
    "            case_features.append(features)\n",
    "            case_targets.append(target_activity)\n",
    "            ground_truth_anomaly.append(noise_flag)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    case_features = np.array(case_features)\n",
    "    case_targets = np.array(case_targets)\n",
    "    ground_truth_anomaly = np.array(ground_truth_anomaly)\n",
    "    print(\"Total cases with at least %s events:\" % (prefix), case_features.shape[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Encode the Features and Target\n",
    "    # ----------------------------\n",
    "    encoder_features = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    X_encoded = encoder_features.fit_transform(case_features)\n",
    "    print(\"Encoded feature shape:\", X_encoded.shape)\n",
    "\n",
    "    # IMPORTANT: Fit LabelEncoder on the full set of target activities (all cases)\n",
    "    target_encoder = LabelEncoder()\n",
    "    target_encoder.fit(case_targets)\n",
    "    y_encoded = target_encoder.transform(case_targets)\n",
    "    full_classes = target_encoder.classes_\n",
    "    # print(\"Full set of event classes (for prefix %s):\" % prefix, full_classes)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Ordered Train/Test Split and Next Event Prediction Model (Stage 1)\n",
    "    # ----------------------------\n",
    "    # Instead of a random split, take the first 80% for training and the remaining 20% for testing.\n",
    "    n_cases = X_encoded.shape[0]\n",
    "    split_index = int(training_size * n_cases)\n",
    "    test_index = split_index\n",
    "    X_train = X_encoded[:split_index]\n",
    "    X_test = X_encoded[test_index:]\n",
    "    y_train = y_encoded[:split_index]\n",
    "    y_test = y_encoded[test_index:]\n",
    "    gt_anomaly_train = ground_truth_anomaly[:split_index]\n",
    "    gt_anomaly_test = ground_truth_anomaly[test_index:]\n",
    "    print(\"Training cases:\", X_train.shape[0], \"Test cases:\", X_test.shape[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Train model\n",
    "    # ----------------------------\n",
    "\n",
    "    # Train a RandomForest classifier with the training set.\n",
    "    rf_model  = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Anomaly Detection (Stage 2) with Dynamic Thresholding\n",
    "    # ----------------------------\n",
    "    # Obtain predicted probabilities for the test set.\n",
    "\n",
    "    normal_loss_dist = normal_loss(rf_model, X_test, y_test)\n",
    "    sorted_normal_loss_dist = np.array([i for i in normal_loss_dist if i <1.1])\n",
    "    sorted_normal_loss_dist = sorted(sorted_normal_loss_dist, reverse=True)\n",
    "\n",
    "    cross_entropy_loss_dist = cross_entropy_loss(rf_model, X_train, y_train)\n",
    "    sorted_cross_entropy_loss_dist = [i for i in cross_entropy_loss_dist if i < log_loss([1,0], [0,1])+1]\n",
    "    sorted_cross_entropy_loss_dist = sorted(sorted_cross_entropy_loss_dist, reverse=True)\n",
    "\n",
    "    sorted_normal_loss_dist,sorted_cross_entropy_loss_dist = get_clean_loss(sorted_normal_loss_dist, sorted_cross_entropy_loss_dist)\n",
    "\n",
    "    diffs = np.diff(sorted_cross_entropy_loss_dist)  # consecutive differences\n",
    "    threshold = -0.02  # e.g., define a large negative drop as dramatic\n",
    "    dramatic_indices = [i for i, d in enumerate(diffs, start=1) if d < threshold]\n",
    "\n",
    "    if len(dramatic_indices) ==0:\n",
    "        adaptive_thr =1\n",
    "    else:\n",
    "        adaptive_thr = sorted_cross_entropy_loss_dist[dramatic_indices[0]]\n",
    "        adaptive_thr_idx = sorted_cross_entropy_loss_dist.index(adaptive_thr)\n",
    "    if anomaly_thr_method == 'fixed':\n",
    "        adaptive_thr = 0.01\n",
    "        predicted_anomaly = (normal_loss_dist > 1-adaptive_thr).astype(int)\n",
    "    elif anomaly_thr_method == 'diff':\n",
    "        predicted_anomaly = (cross_entropy_loss_dist > adaptive_thr).astype(int)\n",
    "        adaptive_thr_dict[prefix] = adaptive_thr\n",
    "\n",
    "    x = np.arange(1, len(sorted_cross_entropy_loss_dist) + 1)\n",
    "    y = sorted_cross_entropy_loss_dist\n",
    "\n",
    "    m = max(50, int(0.05 * len(y)))\n",
    "    while m < len(y) and max(y[:m]) == min(y[:m]):\n",
    "        m += 1 \n",
    "    # 'convex' because the curve bends downward\n",
    "    knee = KneeLocator(x[:m], y[:m], curve='convex', direction='decreasing', S=10)\n",
    "    k_idx = knee.knee  # rank of knee, 1-based\n",
    "    adaptive_thr_kneed = y[k_idx - 1]\n",
    "    print(f\"Kneedle knee at position {k_idx}: threshold={round(adaptive_thr_kneed,3)} Cross entropy diff={round(adaptive_thr,3)}\")\n",
    "    print(f\"Kneedle Normal loss={sorted_normal_loss_dist[k_idx - 1]} Drop diff normal loss={sorted_normal_loss_dist[adaptive_thr_idx]}\")\n",
    "\n",
    "    cross_entropy_loss_dist = cross_entropy_loss(rf_model, X_test, y_test)\n",
    "\n",
    "    predicted_anomaly = (cross_entropy_loss(rf_model, X_test, y_test) > adaptive_thr_kneed).astype(int)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Evaluate the Anomaly Detection\n",
    "    # ----------------------------\n",
    "\n",
    "    classification = classification_report(gt_anomaly_test, predicted_anomaly, output_dict=True)\n",
    "    f1 = classification.get('1', {}).get('f1-score', 0)\n",
    "    support = classification.get('1', {}).get('support', 0)\n",
    "    classification_result[prefix] = classification\n",
    "#     classification_result[prefix]['ROC AUC'] = roc_auc_score(gt_anomaly_test, anom_clf.predict_proba(x_detect_test)[:,1])\n",
    "\n",
    "classification_result = cleaning_cls_result(classification_result)\n",
    "revised_cls_result = {}\n",
    "for i in classification_result.keys():\n",
    "    revised_cls_result[i] = dict()\n",
    "    revised_cls_result[i]['Normal precision'] =classification_result[i]['0']['precision']\n",
    "    revised_cls_result[i]['Normal recall'] =classification_result[i]['0']['recall']\n",
    "    revised_cls_result[i]['Normal f1-score'] =classification_result[i]['0']['f1-score']\n",
    "    revised_cls_result[i]['Normal support'] =classification_result[i]['0']['support']\n",
    "\n",
    "    revised_cls_result[i]['Anomal precision'] =classification_result[i]['1']['precision']\n",
    "    revised_cls_result[i]['Anomal recall'] =classification_result[i]['1']['recall']\n",
    "    revised_cls_result[i]['Anomal f1-score'] =classification_result[i]['1']['f1-score']\n",
    "    revised_cls_result[i]['Anomal support'] =classification_result[i]['1']['support']    \n",
    "\n",
    "    revised_cls_result[i]['Macro precision'] =classification_result[i]['macro avg']['precision']   \n",
    "    revised_cls_result[i]['Macro recall'] =classification_result[i]['macro avg']['recall']   \n",
    "    revised_cls_result[i]['Macro f1-score'] =classification_result[i]['macro avg']['f1-score']   \n",
    "#     revised_cls_result[i]['ROC AUC'] =classification_result[i]['ROC AUC']   \n",
    "result_df = pd.DataFrame.from_dict(revised_cls_result).T\n",
    "result_df.index = result_df.index.set_names(['Prefix length'])\n",
    "result_df = result_df.reset_index(drop=False)\n",
    "# result_file_title = '../result/%s_cross_entropy_%sfold_%s_anomal_thr_result.csv'%(dataset, fold, anomaly_thr_method)\n",
    "# print(result_file_title)\n",
    "result_df\n",
    "# result_df.to_csv(result_file_title, index=False)\n",
    "\n",
    "# loss_prefix_title = '../result/%s_cross_entropy_loss_list.json'%(dataset)\n",
    "# with open(loss_prefix_title, 'w') as f:\n",
    "#     json.dump(adaptive_thr_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee0859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
