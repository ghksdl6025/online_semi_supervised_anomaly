{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ed7bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import datetime\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, log_loss, roc_auc_score\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba23f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, x_test, y_test):\n",
    "    \n",
    "    probs = model.predict_proba(X_test)\n",
    "    \n",
    "    predicted_probs = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if len(idx_arr) == 0:\n",
    "            predicted_probs.append(log_loss(y_true = [1,0], y_pred=[0,1])+1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            true_label_one_hot = np.zeros_like(probs[i])\n",
    "            true_label_one_hot[idx_arr] = 1\n",
    "            predicted_probs.append(log_loss(y_true = true_label_one_hot, y_pred = probs[i]))\n",
    "            \n",
    "    return np.array(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4d6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_loss(model, x_test, y_test):\n",
    "    \n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    predicted_probs = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if len(idx_arr) == 0:\n",
    "            predicted_probs.append(1.1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            \n",
    "            true_label_one_hot = np.zeros_like(probs[i])\n",
    "            true_label_one_hot[idx_arr] = 1\n",
    "            predicted_probs.append(1-probs[i][col_index])\n",
    "            \n",
    "    return np.array(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ece0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform_target(encoder, targets, unknown_value=-1):\n",
    "    classes = set(encoder.classes_)\n",
    "    transformed = []\n",
    "    for t in targets:\n",
    "        if t in classes:\n",
    "            transformed.append(encoder.transform([t])[0])\n",
    "        else:\n",
    "            transformed.append(unknown_value)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3041be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_loss(normal_loss_value, cross_entropy_loss_value):\n",
    "    normal_loss_dist = []\n",
    "    cross_loss_dist = []\n",
    "    for pos, prediction in  enumerate(normal_loss_value):\n",
    "        if prediction != 1:\n",
    "            cross_loss_dist.append(cross_entropy_loss_value[pos])\n",
    "            normal_loss_dist.append(prediction)\n",
    "\n",
    "    return normal_loss_dist, cross_loss_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b091a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cls_result(classification_result):\n",
    "    \n",
    "    for i in classification_result.keys():\n",
    "        print(i, classification_result[i].keys())\n",
    "\n",
    "        if '1' not in classification_result[i].keys():\n",
    "            classification_result[i]['1'] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0.0}\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a89bf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_min_anomalies(gt_labels, num_samples=10, min_anomalies=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly sample `num_samples` indices from gt_labels (0/1 array),\n",
    "    ensuring at least `min_anomalies` true-anomaly (1) indices are included.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_labels : array-like, shape (n_samples,)\n",
    "        Ground-truth labels (0 = normal, 1 = anomaly).\n",
    "    num_samples : int, default=10\n",
    "        Total number of indices to sample.\n",
    "    min_anomalies : int, default=3\n",
    "        Minimum number of anomaly indices to include.\n",
    "    random_state : int or None\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    selected_indices : ndarray, shape (<= num_samples,)\n",
    "        Shuffled indices, containing at least `min_anomalies` anomalies\n",
    "        (or as many as available if fewer exist).\n",
    "    \"\"\"\n",
    "    gt_labels = np.asarray(gt_labels)\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # locate anomaly vs normal indices\n",
    "    anomaly_idx = np.where(gt_labels == 1)[0]\n",
    "    normal_idx  = np.where(gt_labels == 0)[0]\n",
    "\n",
    "    # determine how many anomalies we can pick\n",
    "    n_anom = min(len(anomaly_idx), min_anomalies)\n",
    "    # pick anomalies without replacement\n",
    "    picked_anom = np.random.choice(anomaly_idx, n_anom, replace=False) if n_anom > 0 else np.array([], dtype=int)\n",
    "\n",
    "    # fill the rest from normals\n",
    "    n_normal = num_samples - n_anom\n",
    "    n_normal = min(n_normal, len(normal_idx))\n",
    "    picked_norm = np.random.choice(normal_idx, n_normal, replace=False) if n_normal > 0 else np.array([], dtype=int)\n",
    "\n",
    "    # combine and shuffle\n",
    "    selected = np.concatenate([picked_anom, picked_norm])\n",
    "    np.random.shuffle(selected)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7021ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Read and Process the Data\n",
    "# ----------------------------\n",
    "dataset = '0.099_sample.csv'\n",
    "df = pd.read_csv(\"../data/%s\" % (dataset))\n",
    "df = df.sort_values(by='Timestamp')\n",
    "# Process the 'noise' column:\n",
    "# - If NaN, assume Normal (0).\n",
    "# - Otherwise, treat True/1/'True' as anomaly (1); everything else as Normal (0).\n",
    "df['noise'] = df['noise'].fillna(0).apply(lambda x: 1 if (x == True or x == 1 or x == 'True' or x=='true') else 0)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(dataset)\n",
    "# Calculate the cutoff time (e.g., the median of all timestamps)\n",
    "cutoff_time = df['Timestamp'].median()\n",
    "\n",
    "anomaly_f1_list = []\n",
    "anomaly_support_list = []\n",
    "prefix_range = range(2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "418e86ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training window size: 0.8\n",
      "Total cases with at least 2 events: 500\n",
      "Encoded feature shape: (500, 17)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 2 ---\n",
      "Classification Report: F1-score = 1.0, Support = 11.0\n",
      "Total cases with at least 3 events: 500\n",
      "Encoded feature shape: (500, 34)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 3 ---\n",
      "Classification Report: F1-score = 0.8333333333333334, Support = 7.0\n",
      "Total cases with at least 4 events: 500\n",
      "Encoded feature shape: (500, 52)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 4 ---\n",
      "Classification Report: F1-score = 0.17391304347826086, Support = 9.0\n",
      "Total cases with at least 5 events: 500\n",
      "Encoded feature shape: (500, 70)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 5 ---\n",
      "Classification Report: F1-score = 0.12698412698412698, Support = 10.0\n",
      "Total cases with at least 6 events: 500\n",
      "Encoded feature shape: (500, 88)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 6 ---\n",
      "Classification Report: F1-score = 0.23728813559322035, Support = 12.0\n",
      "Total cases with at least 7 events: 500\n",
      "Encoded feature shape: (500, 105)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 7 ---\n",
      "Classification Report: F1-score = 0.2857142857142857, Support = 13.0\n",
      "Total cases with at least 8 events: 500\n",
      "Encoded feature shape: (500, 123)\n",
      "Training cases: 400 Test cases: 100\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 8 ---\n",
      "Classification Report: F1-score = 0.2222222222222222, Support = 8.0\n",
      "Total cases with at least 9 events: 446\n",
      "Encoded feature shape: (446, 141)\n",
      "Training cases: 356 Test cases: 90\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 9 ---\n",
      "Classification Report: F1-score = 0.23076923076923078, Support = 9.0\n",
      "Total cases with at least 10 events: 402\n",
      "Encoded feature shape: (402, 158)\n",
      "Training cases: 321 Test cases: 81\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 10 ---\n",
      "Classification Report: F1-score = 0.28, Support = 8.0\n",
      "Total cases with at least 11 events: 373\n",
      "Encoded feature shape: (373, 171)\n",
      "Training cases: 298 Test cases: 75\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 11 ---\n",
      "Classification Report: F1-score = 0.058823529411764705, Support = 5.0\n",
      "Total cases with at least 12 events: 318\n",
      "Encoded feature shape: (318, 187)\n",
      "Training cases: 254 Test cases: 64\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 12 ---\n",
      "Classification Report: F1-score = 0.22857142857142856, Support = 5.0\n",
      "Total cases with at least 13 events: 241\n",
      "Encoded feature shape: (241, 199)\n",
      "Training cases: 192 Test cases: 49\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 13 ---\n",
      "Classification Report: F1-score = 0.4444444444444444, Support = 8.0\n",
      "Total cases with at least 14 events: 201\n",
      "Encoded feature shape: (201, 203)\n",
      "Training cases: 160 Test cases: 41\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 14 ---\n",
      "Classification Report: F1-score = 0.21428571428571427, Support = 3.0\n",
      "Total cases with at least 15 events: 171\n",
      "Encoded feature shape: (171, 206)\n",
      "Training cases: 136 Test cases: 35\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 15 ---\n",
      "Classification Report: F1-score = 0.0, Support = 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefix length</th>\n",
       "      <th>Normal precision</th>\n",
       "      <th>Normal recall</th>\n",
       "      <th>Normal f1-score</th>\n",
       "      <th>Normal support</th>\n",
       "      <th>Anomal precision</th>\n",
       "      <th>Anomal recall</th>\n",
       "      <th>Anomal f1-score</th>\n",
       "      <th>Anomal support</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Macro f1-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.734255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.892655</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.530731</td>\n",
       "      <td>0.545177</td>\n",
       "      <td>0.533284</td>\n",
       "      <td>0.562271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.598540</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.473906</td>\n",
       "      <td>0.427778</td>\n",
       "      <td>0.362762</td>\n",
       "      <td>0.451667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.527298</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>0.459070</td>\n",
       "      <td>0.591856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.528736</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.610522</td>\n",
       "      <td>0.478624</td>\n",
       "      <td>0.738285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.543758</td>\n",
       "      <td>0.638587</td>\n",
       "      <td>0.498208</td>\n",
       "      <td>0.529891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.543210</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.537853</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.459135</td>\n",
       "      <td>0.665981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.570513</td>\n",
       "      <td>0.697774</td>\n",
       "      <td>0.479286</td>\n",
       "      <td>0.699486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.473763</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.391481</td>\n",
       "      <td>0.495714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.551961</td>\n",
       "      <td>0.679661</td>\n",
       "      <td>0.469124</td>\n",
       "      <td>0.577966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.648718</td>\n",
       "      <td>0.676829</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.653963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.403439</td>\n",
       "      <td>0.627193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.323529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prefix length  Normal precision  Normal recall  Normal f1-score  \\\n",
       "0               2          1.000000       1.000000         1.000000   \n",
       "1               3          0.978947       1.000000         0.989362   \n",
       "2               4          0.918605       0.868132         0.892655   \n",
       "3               5          0.872340       0.455556         0.598540   \n",
       "4               6          0.905660       0.545455         0.680851   \n",
       "5               7          0.920000       0.528736         0.671533   \n",
       "6               8          0.952381       0.652174         0.774194   \n",
       "7               9          0.936170       0.543210         0.687500   \n",
       "8              10          0.974359       0.520548         0.678571   \n",
       "9              11          0.913043       0.600000         0.724138   \n",
       "10             12          0.970588       0.559322         0.709677   \n",
       "11             13          0.897436       0.853659         0.875000   \n",
       "12             14          1.000000       0.421053         0.592593   \n",
       "13             15          0.937500       0.441176         0.600000   \n",
       "\n",
       "    Normal support  Anomal precision  Anomal recall  Anomal f1-score  \\\n",
       "0             89.0          1.000000       1.000000         1.000000   \n",
       "1             93.0          1.000000       0.714286         0.833333   \n",
       "2             91.0          0.142857       0.222222         0.173913   \n",
       "3             90.0          0.075472       0.400000         0.126984   \n",
       "4             88.0          0.148936       0.583333         0.237288   \n",
       "5             87.0          0.180000       0.692308         0.285714   \n",
       "6             92.0          0.135135       0.625000         0.222222   \n",
       "7             81.0          0.139535       0.666667         0.230769   \n",
       "8             73.0          0.166667       0.875000         0.280000   \n",
       "9             70.0          0.034483       0.200000         0.058824   \n",
       "10            59.0          0.133333       0.800000         0.228571   \n",
       "11            41.0          0.400000       0.500000         0.444444   \n",
       "12            38.0          0.120000       1.000000         0.214286   \n",
       "13            34.0          0.000000       0.000000         0.000000   \n",
       "\n",
       "    Anomal support  Macro precision  Macro recall  Macro f1-score   ROC AUC  \n",
       "0             11.0         1.000000      1.000000        1.000000  1.000000  \n",
       "1              7.0         0.989474      0.857143        0.911348  0.734255  \n",
       "2              9.0         0.530731      0.545177        0.533284  0.562271  \n",
       "3             10.0         0.473906      0.427778        0.362762  0.451667  \n",
       "4             12.0         0.527298      0.564394        0.459070  0.591856  \n",
       "5             13.0         0.550000      0.610522        0.478624  0.738285  \n",
       "6              8.0         0.543758      0.638587        0.498208  0.529891  \n",
       "7              9.0         0.537853      0.604938        0.459135  0.665981  \n",
       "8              8.0         0.570513      0.697774        0.479286  0.699486  \n",
       "9              5.0         0.473763      0.400000        0.391481  0.495714  \n",
       "10             5.0         0.551961      0.679661        0.469124  0.577966  \n",
       "11             8.0         0.648718      0.676829        0.659722  0.653963  \n",
       "12             3.0         0.560000      0.710526        0.403439  0.627193  \n",
       "13             1.0         0.468750      0.220588        0.300000  0.323529  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = 0.8\n",
    "print('Training window size: %s' % (training_size))\n",
    "loss_prefix_dict =dict()\n",
    "classification_result = dict()\n",
    "for prefix in prefix_range:    \n",
    "    # Extract per case:\n",
    "    # - The first (prefix-1) events (activities) as features.\n",
    "    # - The prefix-th event's activity as the target.\n",
    "    # - The prefix-th event's noise flag as the ground truth anomaly.\n",
    "    case_features = []\n",
    "    case_targets = []\n",
    "    ground_truth_anomaly = []\n",
    "\n",
    "    for case_id, group in df.groupby('ID'):\n",
    "        group = group.sort_index()  # assuming the order in the file is the event order\n",
    "        if len(group) >= prefix:\n",
    "            events = group['Activity'].values  # adjust 'Activity' if needed\n",
    "            features = events[:prefix-1]\n",
    "            target_activity = events[prefix-1]  # prefix-th event's activity\n",
    "            noise_flag = group['noise'].iloc[prefix-1]\n",
    "\n",
    "            case_features.append(features)\n",
    "            case_targets.append(target_activity)\n",
    "            ground_truth_anomaly.append(noise_flag)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    case_features = np.array(case_features)\n",
    "    case_targets = np.array(case_targets)\n",
    "    ground_truth_anomaly = np.array(ground_truth_anomaly)\n",
    "    print(\"Total cases with at least %s events:\" % (prefix), case_features.shape[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Encode the Features and Target\n",
    "    # ----------------------------\n",
    "    encoder_features = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    X_encoded = encoder_features.fit_transform(case_features)\n",
    "    print(\"Encoded feature shape:\", X_encoded.shape)\n",
    "\n",
    "    # IMPORTANT: Fit LabelEncoder on the full set of target activities (all cases)\n",
    "    target_encoder = LabelEncoder()\n",
    "    target_encoder.fit(case_targets)\n",
    "    y_encoded = target_encoder.transform(case_targets)\n",
    "    full_classes = target_encoder.classes_\n",
    "    # print(\"Full set of event classes (for prefix %s):\" % prefix, full_classes)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Ordered Train/Test Split and Next Event Prediction Model (Stage 1)\n",
    "    # ----------------------------\n",
    "    # Instead of a random split, take the first 80% for training and the remaining 20% for testing.\n",
    "    n_cases = X_encoded.shape[0]\n",
    "    split_index = int(training_size * n_cases)\n",
    "    test_index = split_index\n",
    "    X_train = X_encoded[:split_index]\n",
    "    X_test = X_encoded[test_index:]\n",
    "    y_train = y_encoded[:split_index]\n",
    "    y_test = y_encoded[test_index:]\n",
    "    gt_anomaly_train = ground_truth_anomaly[:split_index]\n",
    "    gt_anomaly_test = ground_truth_anomaly[test_index:]\n",
    "    print(\"Training cases:\", X_train.shape[0], \"Test cases:\", X_test.shape[0])\n",
    "\n",
    "\n",
    "    # Train a RandomForest classifier with the training set.\n",
    "    rf_model  = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    # Now rf_model.classes_ should include all classes from full_classes.\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4-1: Random Anomaly is Known with CE\n",
    "    # ----------------------------\n",
    "    \n",
    "    expert_anomaly_indices = sample_with_min_anomalies(\n",
    "        gt_labels=gt_anomaly_train,\n",
    "        num_samples=20,\n",
    "        min_anomalies=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    ce_loss = cross_entropy_loss(model=rf_model, x_test = [X_train[idx] for idx in expert_anomaly_indices], \n",
    "                                 y_test = [y_train[idx] for idx in expert_anomaly_indices])\n",
    "    # Modify training set for anomaly detection classifier\n",
    "    x_detect_train = []\n",
    "    y_detect_train = []\n",
    "    for pos, idx in enumerate(expert_anomaly_indices):\n",
    "        new_training = np.array([])\n",
    "        new_training = np.append(new_training, X_train[idx])\n",
    "        new_training = np.append(new_training, y_train[idx])\n",
    "        new_training = np.concatenate((new_training, rf_model.predict_proba(X_train[idx].reshape(1,-1))[0]))\n",
    "        new_training = np.append(new_training, ce_loss[pos])\n",
    "        x_detect_train.append(new_training)\n",
    "        y_detect_train.append(gt_anomaly_train[idx])\n",
    "        \n",
    "    # ----------------------------\n",
    "    # Step 5: Making training set for the anomaly detection classifier \n",
    "    # ----------------------------\n",
    "    anom_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     anom_clf = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    anom_clf.fit(x_detect_train, y_detect_train)\n",
    "\n",
    "\n",
    "    ce_loss = cross_entropy_loss(model=rf_model, x_test = X_test, y_test = y_test)\n",
    "    # Modify test set for anomaly detection classifier\n",
    "    x_detect_test = []\n",
    "    y_detect_test = []\n",
    "    for idx in range(len(X_test)):\n",
    "        new_test = np.array([])\n",
    "        new_test = np.append(new_test, X_test[idx])\n",
    "        new_test = np.append(new_test, y_test[idx])\n",
    "        new_test = np.concatenate((new_test, rf_model.predict_proba(X_test[idx].reshape(1,-1))[0]))\n",
    "        new_test = np.append(new_test, ce_loss[idx])\n",
    "        x_detect_test.append(new_test)\n",
    "        y_detect_test.append(gt_anomaly_test[idx])\n",
    "\n",
    "\n",
    "    # Train and predict\n",
    "    predicted_anomaly = anom_clf.predict(x_detect_test)\n",
    "    # ----------------------------\n",
    "    # Step 5: Evaluate the Anomaly Detection\n",
    "    # ----------------------------\n",
    "    print(\"\\n--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix %s ---\" % prefix)\n",
    "    classification = classification_report(gt_anomaly_test, predicted_anomaly, output_dict=True)\n",
    "    f1 = classification.get('1', {}).get('f1-score', 0)\n",
    "    support = classification.get('1', {}).get('support', 0)\n",
    "    classification_result[prefix] = classification\n",
    "    print(f\"Classification Report: F1-score = {f1}, Support = {support}\")\n",
    "    classification_result[prefix]['ROC AUC'] = roc_auc_score(gt_anomaly_test, anom_clf.predict_proba(x_detect_test)[:,1])\n",
    "revised_cls_result = {}\n",
    "for i in classification_result.keys():\n",
    "    revised_cls_result[i] = dict()\n",
    "    revised_cls_result[i]['Normal precision'] =classification_result[i]['0']['precision']\n",
    "    revised_cls_result[i]['Normal recall'] =classification_result[i]['0']['recall']\n",
    "    revised_cls_result[i]['Normal f1-score'] =classification_result[i]['0']['f1-score']\n",
    "    revised_cls_result[i]['Normal support'] =classification_result[i]['0']['support']\n",
    "\n",
    "    revised_cls_result[i]['Anomal precision'] =classification_result[i]['1']['precision']\n",
    "    revised_cls_result[i]['Anomal recall'] =classification_result[i]['1']['recall']\n",
    "    revised_cls_result[i]['Anomal f1-score'] =classification_result[i]['1']['f1-score']\n",
    "    revised_cls_result[i]['Anomal support'] =classification_result[i]['1']['support']    \n",
    "\n",
    "    revised_cls_result[i]['Macro precision'] =classification_result[i]['macro avg']['precision']   \n",
    "    revised_cls_result[i]['Macro recall'] =classification_result[i]['macro avg']['recall']   \n",
    "    revised_cls_result[i]['Macro f1-score'] =classification_result[i]['macro avg']['f1-score']   \n",
    "    revised_cls_result[i]['ROC AUC'] =classification_result[i]['ROC AUC']   \n",
    "result_df = pd.DataFrame.from_dict(revised_cls_result).T\n",
    "result_df.index = result_df.index.set_names(['Prefix length'])\n",
    "result_df = result_df.reset_index(drop=False)\n",
    "# result_file_title = '../result/%s_cross_entropy_%s_anomal_thr_result.csv'%(dataset, anomaly_thr_method)\n",
    "# print(result_file_title)\n",
    "result_df\n",
    "#     result_df.to_csv(result_file_title, index=False)\n",
    "\n",
    "# loss_prefix_title = '../result/%s_cross_entropy_loss_result.json'%(dataset)\n",
    "# with open(loss_prefix_title, 'w') as f:\n",
    "#     json.dump(loss_prefix_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8d166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
