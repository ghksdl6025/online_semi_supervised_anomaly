{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82eade06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import datetime\n",
    "import time\n",
    "from collections import Counter, defaultdict, deque\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, log_loss, roc_auc_score\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b283123-7fd9-467c-9243-b588eff6905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# suppress only the “y_pred values do not sum to one” warning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*y_pred values do not sum to one.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ac0c10-ccfa-4abc-adf9-a76c8e0d9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    For each sample i:\n",
    "      loss_i = -∑_c [1{c = y_true_i} · log P_model(c | x_i)]\n",
    "    If the true label isn’t in model.classes_, returns a default high loss.\n",
    "    Works for any len(x_test) >= 1, including the single-class case.\n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(x_test)\n",
    "    default = log_loss([[1, 0]], [[0, 1]]) + 1  # fallback loss\n",
    "\n",
    "    losses = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        sample_probs = probs[i]\n",
    "        classes = model.classes_\n",
    "\n",
    "        # if only one class in the model\n",
    "        if sample_probs.size == 1:\n",
    "            if classes[0] == true_label:\n",
    "                losses.append(0.0)  # perfect prediction\n",
    "            else:\n",
    "                losses.append(default)\n",
    "            continue\n",
    "\n",
    "        # find index of the true label\n",
    "        idx_arr = np.where(classes == true_label)[0]\n",
    "        if idx_arr.size == 0:\n",
    "            losses.append(default)\n",
    "        else:\n",
    "            y_true_onehot = np.zeros_like(sample_probs)\n",
    "            y_true_onehot[idx_arr[0]] = 1\n",
    "\n",
    "            # normalize just in case\n",
    "            sample_probs = sample_probs / sample_probs.sum()\n",
    "            y_true_onehot = y_true_onehot / y_true_onehot.sum()\n",
    "\n",
    "            loss_i = log_loss([y_true_onehot], [sample_probs])\n",
    "            losses.append(loss_i)\n",
    "\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db274a0-0b5e-4743-b3b5-e5be052bc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_loss(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    For each sample i:\n",
    "      loss_i = 1 - P_model(y_true_i | x_i)\n",
    "    If the true label isn’t in model.classes_, we return 1.1 as before.\n",
    "    Works for any len(x_test) >= 1.\n",
    "    \"\"\"\n",
    "    # predict_proba returns shape (n_samples, n_classes)\n",
    "    probs = model.predict_proba(x_test)\n",
    "    \n",
    "    losses = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        sample_probs = probs[i]\n",
    "        # find index of the true label in model.classes_\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if idx_arr.size == 0:\n",
    "            losses.append(1.1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            losses.append(1 - sample_probs[col_index])\n",
    "    \n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24711632-7cbd-4d1b-bc42-19e79f68840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform_target(encoder, targets, unknown_value=-1):\n",
    "    classes = set(encoder.classes_)\n",
    "    transformed = []\n",
    "    for t in targets:\n",
    "        if t in classes:\n",
    "            transformed.append(encoder.transform([t])[0])\n",
    "        else:\n",
    "            transformed.append(unknown_value)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96178cae-ff9a-4b8b-a612-7c2a16cce9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_loss(normal_loss_value, cross_entropy_loss_value):\n",
    "    normal_loss_dist = []\n",
    "    cross_loss_dist = []\n",
    "    for pos, prediction in  enumerate(normal_loss_value):\n",
    "        if prediction != 1:\n",
    "            cross_loss_dist.append(cross_entropy_loss_value[pos])\n",
    "            normal_loss_dist.append(prediction)\n",
    "\n",
    "    return np.array(normal_loss_dist), np.array(cross_loss_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ecb9aa-e714-4693-8b5f-2969e7999b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cls_result(classification_result):\n",
    "    \n",
    "    for i in classification_result.keys():\n",
    "        print(i, classification_result[i].keys())\n",
    "\n",
    "        if '1' not in classification_result[i].keys():\n",
    "            classification_result[i]['1'] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0.0}\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45c8ef0-4711-45c4-88d0-fb1c2cb72f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_min_anomalies(gt_labels, num_samples=10, min_anomalies=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly sample `num_samples` indices from gt_labels (0/1 array),\n",
    "    ensuring at least `min_anomalies` true-anomaly (1) indices are included.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_labels : array-like, shape (n_samples,)\n",
    "        Ground-truth labels (0 = normal, 1 = anomaly).\n",
    "    num_samples : int, default=10\n",
    "        Total number of indices to sample.\n",
    "    min_anomalies : int, default=3\n",
    "        Minimum number of anomaly indices to include.\n",
    "    random_state : int or None\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    selected_indices : ndarray, shape (<= num_samples,)\n",
    "        Shuffled indices, containing at least `min_anomalies` anomalies\n",
    "        (or as many as available if fewer exist).\n",
    "    \"\"\"\n",
    "    gt_labels = np.asarray(gt_labels)\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # locate anomaly vs normal indices\n",
    "    anomaly_idx = np.where(gt_labels == 1)[0]\n",
    "    normal_idx  = np.where(gt_labels == 0)[0]\n",
    "\n",
    "    # determine how many anomalies we can pick\n",
    "    n_anom = min(len(anomaly_idx), min_anomalies)\n",
    "    # pick anomalies without replacement\n",
    "    picked_anom = np.random.choice(anomaly_idx, n_anom, replace=False) if n_anom > 0 else np.array([], dtype=int)\n",
    "\n",
    "    # fill the rest from normals\n",
    "    n_normal = num_samples - n_anom\n",
    "    n_normal = min(n_normal, len(normal_idx))\n",
    "    picked_norm = np.random.choice(normal_idx, n_normal, replace=False) if n_normal > 0 else np.array([], dtype=int)\n",
    "\n",
    "    # combine and shuffle\n",
    "    selected = np.concatenate([picked_anom, picked_norm])\n",
    "    np.random.shuffle(selected)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc0b8ebc-5f8c-4623-ad4b-a52f864c9744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099_noise.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Read and Process the Data\n",
    "# ----------------------------\n",
    "dataset = '0.099_noise.csv'\n",
    "df = pd.read_csv(\"../data/%s\" % (dataset))\n",
    "df = df.sort_values(by='Timestamp')\n",
    "# Process the 'noise' column:\n",
    "# - If NaN, assume Normal (0).\n",
    "# - Otherwise, treat True/1/'True' as anomaly (1); everything else as Normal (0).\n",
    "df['noise'] = df['noise'].fillna(0).apply(lambda x: 1 if (x == True or x == 1 or x == 'True' or x=='true') else 0)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(dataset)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Prefixes & global window settings\n",
    "# ----------------------------\n",
    "prefix_range = range(2, 35)   # prefix lengths 2..15\n",
    "WINDOW_EVENTS = 2500          # keep the last 2 500 raw events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cde075b-d685-4702-9b22-1dcfe37971a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Pre-fit encoders\n",
    "# ----------------------------\n",
    "all_activities = df[\"Activity\"].unique()\n",
    "le = LabelEncoder().fit(all_activities)\n",
    "\n",
    "# build one OHE per prefix length\n",
    "ohe_dict = {}\n",
    "for p in prefix_range:\n",
    "    # we need p-1 columns, each with same category set\n",
    "    dummy = np.array([[act] * (p - 1) for act in all_activities])\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    ohe.fit(dummy)\n",
    "    ohe_dict[p] = ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ef01fa-2166-42c1-9c18-f48e98a20d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) Prepare global window & per-prefix buffers\n",
    "# ----------------------------\n",
    "# a single sliding window of the last WINDOW_EVENTS raw events\n",
    "global_events = deque(maxlen=WINDOW_EVENTS)\n",
    "\n",
    "# per-prefix feature/label/noise buffers (we’ll manage evictions manually)\n",
    "buffers = {}\n",
    "for p in prefix_range:\n",
    "    buffers[p] = {\n",
    "        \"X\": deque(),               # feature vectors for prefix length p\n",
    "        \"y\": deque(),               # target labels\n",
    "        \"noise\": deque(),           # noise/anomaly flags\n",
    "        \"model\": None,              \n",
    "        \"filled\": False,            \n",
    "        \"update_counter\": 0,        \n",
    "        # retrain every half-window by default; adjust if you like\n",
    "        \"retrain_batch\": max(1, WINDOW_EVENTS // 2)\n",
    "    }\n",
    "\n",
    "case_events = defaultdict(list)    # to accumulate per-case histories\n",
    "online_reports = []                # to collect results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "060df11a-be96-4e54-998a-f8bc68850f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 10000/79441 rows (12.6%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 20000/79441 rows (25.2%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 30000/79441 rows (37.8%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 40000/79441 rows (50.4%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 50000/79441 rows (62.9%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 60000/79441 rows (75.5%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 70000/79441 rows (88.1%)\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "=== Global retrain of all prefix models ===\n",
      "Processed 79441/79441 rows (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5) Simulate streaming & online learning\n",
    "# ----------------------------\n",
    "total = len(df)\n",
    "\n",
    "# global retrain trigger\n",
    "global_update_counter = 0\n",
    "global_retrain_batch = WINDOW_EVENTS // 2   # 1250\n",
    "\n",
    "for i, (_, row) in enumerate(df.iterrows(), start=1):\n",
    "    # progress logging\n",
    "    if i % 10000 == 0 or i == total:\n",
    "        pct = i / total * 100\n",
    "        print(f\"Processed {i}/{total} rows ({pct:.1f}%)\")\n",
    "\n",
    "    # accumulate per-case history\n",
    "    cid = row[\"Case ID\"]\n",
    "    case_events[cid].append(row)\n",
    "    cur_len = len(case_events[cid])\n",
    "\n",
    "    # only act when a case reaches a prefix length p\n",
    "    for p in prefix_range:\n",
    "        if cur_len != p:\n",
    "            continue\n",
    "\n",
    "        # build feature, target, noise flag\n",
    "        group = case_events[cid]\n",
    "        feats = [e[\"Activity\"] for e in group[: p - 1]]\n",
    "        target = group[p - 1][\"Activity\"]\n",
    "        noise_flag = group[p - 1][\"noise\"]\n",
    "\n",
    "        # 1) Prequential prediction if the model is ready\n",
    "        if buffers[p][\"filled\"]:\n",
    "            rf = buffers[p][\"model\"]\n",
    "            Xp = ohe_dict[p].transform([feats])\n",
    "            encoded_target = le.transform([target])[0]\n",
    "\n",
    "            nl  = normal_loss(rf, Xp, [encoded_target] )\n",
    "            cel = cross_entropy_loss(rf, Xp, [encoded_target])\n",
    "            pred_anom = (nl > 1 - 0.01).astype(int)\n",
    "\n",
    "            online_reports.append({\n",
    "                \"i\":          i,\n",
    "                \"prefix\":     p,\n",
    "                \"case_id\":    cid,\n",
    "                \"true_noise\": noise_flag,\n",
    "                \"pred_noise\": int(pred_anom[0]),\n",
    "                \"nap_prob\":    nl,\n",
    "                \"nap_class\":  rf.classes_,\n",
    "                \"predict_act\":rf.predict(Xp),\n",
    "                \"actual_act\": le.transform([target])[0]\n",
    "       \n",
    "            })\n",
    "\n",
    "        # 2) Slide the global window: if full, peek the to-be-dropped item\n",
    "        dropped = None\n",
    "        if len(global_events) == WINDOW_EVENTS:\n",
    "            dropped = global_events[0]   # this will be evicted on append()\n",
    "\n",
    "        # append new sample to the global window\n",
    "        Xp = ohe_dict[p].transform([feats]).ravel()\n",
    "        yp = le.transform([target])[0]\n",
    "        global_events.append((p, Xp, yp, noise_flag))\n",
    "\n",
    "        # add to this prefix’s buffer\n",
    "        buffers[p][\"X\"].append(Xp)\n",
    "        buffers[p][\"y\"].append(yp)\n",
    "        buffers[p][\"noise\"].append(noise_flag)\n",
    "\n",
    "        # manually evict from the old prefix buffer if needed\n",
    "        if dropped is not None:\n",
    "            old_p, old_Xp, old_yp, old_noise = dropped\n",
    "            buffers[old_p][\"X\"].popleft()\n",
    "            buffers[old_p][\"y\"].popleft()\n",
    "            buffers[old_p][\"noise\"].popleft()\n",
    "\n",
    "          # 3) Initial training once buffer non-empty (or however you prefer)\n",
    "        if buffers[p][\"model\"] is None:\n",
    "            Xw = np.vstack(buffers[p][\"X\"])\n",
    "            yw = np.array(buffers[p][\"y\"])\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=50,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf.fit(Xw, yw)\n",
    "            buffers[p][\"model\"] = rf\n",
    "            buffers[p][\"filled\"] = True\n",
    "\n",
    "        # 4) Global retrain trigger\n",
    "        global_update_counter += 1\n",
    "        if global_update_counter >= global_retrain_batch:\n",
    "            print(\"=== Global retrain of all prefix models ===\")\n",
    "            for q in prefix_range:\n",
    "                if len(buffers[q][\"X\"]) == 0:\n",
    "                    continue\n",
    "                Xw = np.vstack(buffers[q][\"X\"])\n",
    "                yw = np.array(buffers[q][\"y\"])\n",
    "                rf = RandomForestClassifier(\n",
    "                    n_estimators=50,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                rf.fit(Xw, yw)\n",
    "                buffers[q][\"model\"] = rf\n",
    "            global_update_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d8e417-0463-4985-92e3-0c8a3849ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prefix 2 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      4462\n",
      "           1       1.00      0.74      0.85       537\n",
      "\n",
      "    accuracy                           0.97      4999\n",
      "   macro avg       0.98      0.87      0.92      4999\n",
      "weighted avg       0.97      0.97      0.97      4999\n",
      "\n",
      "\n",
      "--- Prefix 3 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      4549\n",
      "           1       0.96      0.74      0.83       450\n",
      "\n",
      "    accuracy                           0.97      4999\n",
      "   macro avg       0.97      0.87      0.91      4999\n",
      "weighted avg       0.97      0.97      0.97      4999\n",
      "\n",
      "\n",
      "--- Prefix 4 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      4528\n",
      "           1       0.82      0.76      0.79       471\n",
      "\n",
      "    accuracy                           0.96      4999\n",
      "   macro avg       0.90      0.87      0.88      4999\n",
      "weighted avg       0.96      0.96      0.96      4999\n",
      "\n",
      "\n",
      "--- Prefix 5 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      4480\n",
      "           1       0.75      0.75      0.75       519\n",
      "\n",
      "    accuracy                           0.95      4999\n",
      "   macro avg       0.86      0.86      0.86      4999\n",
      "weighted avg       0.95      0.95      0.95      4999\n",
      "\n",
      "\n",
      "--- Prefix 6 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      4549\n",
      "           1       0.76      0.78      0.77       450\n",
      "\n",
      "    accuracy                           0.96      4999\n",
      "   macro avg       0.87      0.88      0.87      4999\n",
      "weighted avg       0.96      0.96      0.96      4999\n",
      "\n",
      "\n",
      "--- Prefix 7 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      4506\n",
      "           1       0.79      0.78      0.78       493\n",
      "\n",
      "    accuracy                           0.96      4999\n",
      "   macro avg       0.88      0.88      0.88      4999\n",
      "weighted avg       0.96      0.96      0.96      4999\n",
      "\n",
      "\n",
      "--- Prefix 8 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      4506\n",
      "           1       0.72      0.84      0.77       493\n",
      "\n",
      "    accuracy                           0.95      4999\n",
      "   macro avg       0.85      0.90      0.87      4999\n",
      "weighted avg       0.96      0.95      0.95      4999\n",
      "\n",
      "\n",
      "--- Prefix 9 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      4514\n",
      "           1       0.72      0.81      0.76       485\n",
      "\n",
      "    accuracy                           0.95      4999\n",
      "   macro avg       0.85      0.89      0.87      4999\n",
      "weighted avg       0.95      0.95      0.95      4999\n",
      "\n",
      "\n",
      "--- Prefix 10 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      4532\n",
      "           1       0.71      0.78      0.75       467\n",
      "\n",
      "    accuracy                           0.95      4999\n",
      "   macro avg       0.85      0.87      0.86      4999\n",
      "weighted avg       0.95      0.95      0.95      4999\n",
      "\n",
      "\n",
      "--- Prefix 11 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96      4089\n",
      "           1       0.58      0.74      0.65       432\n",
      "\n",
      "    accuracy                           0.92      4521\n",
      "   macro avg       0.78      0.84      0.81      4521\n",
      "weighted avg       0.93      0.92      0.93      4521\n",
      "\n",
      "\n",
      "--- Prefix 12 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      3669\n",
      "           1       0.60      0.71      0.65       343\n",
      "\n",
      "    accuracy                           0.93      4012\n",
      "   macro avg       0.79      0.83      0.81      4012\n",
      "weighted avg       0.94      0.93      0.94      4012\n",
      "\n",
      "\n",
      "--- Prefix 13 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      3447\n",
      "           1       0.55      0.62      0.59       346\n",
      "\n",
      "    accuracy                           0.92      3793\n",
      "   macro avg       0.76      0.79      0.77      3793\n",
      "weighted avg       0.92      0.92      0.92      3793\n",
      "\n",
      "\n",
      "--- Prefix 14 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      2961\n",
      "           1       0.45      0.65      0.53       290\n",
      "\n",
      "    accuracy                           0.90      3251\n",
      "   macro avg       0.71      0.79      0.74      3251\n",
      "weighted avg       0.92      0.90      0.91      3251\n",
      "\n",
      "\n",
      "--- Prefix 15 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94      2318\n",
      "           1       0.41      0.59      0.48       229\n",
      "\n",
      "    accuracy                           0.89      2547\n",
      "   macro avg       0.68      0.75      0.71      2547\n",
      "weighted avg       0.91      0.89      0.89      2547\n",
      "\n",
      "\n",
      "--- Prefix 16 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92      1935\n",
      "           1       0.33      0.54      0.41       183\n",
      "\n",
      "    accuracy                           0.87      2118\n",
      "   macro avg       0.64      0.72      0.67      2118\n",
      "weighted avg       0.90      0.87      0.88      2118\n",
      "\n",
      "\n",
      "--- Prefix 17 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      1604\n",
      "           1       0.29      0.55      0.38       164\n",
      "\n",
      "    accuracy                           0.83      1768\n",
      "   macro avg       0.62      0.71      0.64      1768\n",
      "weighted avg       0.89      0.83      0.85      1768\n",
      "\n",
      "\n",
      "--- Prefix 18 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.87      1283\n",
      "           1       0.20      0.45      0.28       129\n",
      "\n",
      "    accuracy                           0.79      1412\n",
      "   macro avg       0.57      0.63      0.58      1412\n",
      "weighted avg       0.87      0.79      0.82      1412\n",
      "\n",
      "\n",
      "--- Prefix 19 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85      1062\n",
      "           1       0.16      0.51      0.25        94\n",
      "\n",
      "    accuracy                           0.74      1156\n",
      "   macro avg       0.55      0.64      0.55      1156\n",
      "weighted avg       0.88      0.74      0.80      1156\n",
      "\n",
      "\n",
      "--- Prefix 20 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.81       845\n",
      "           1       0.17      0.51      0.26        97\n",
      "\n",
      "    accuracy                           0.70       942\n",
      "   macro avg       0.55      0.62      0.54       942\n",
      "weighted avg       0.85      0.70      0.76       942\n",
      "\n",
      "\n",
      "--- Prefix 21 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.78       679\n",
      "           1       0.13      0.52      0.21        65\n",
      "\n",
      "    accuracy                           0.66       744\n",
      "   macro avg       0.53      0.60      0.49       744\n",
      "weighted avg       0.87      0.66      0.73       744\n",
      "\n",
      "\n",
      "--- Prefix 22 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.62      0.75       565\n",
      "           1       0.08      0.53      0.14        36\n",
      "\n",
      "    accuracy                           0.62       601\n",
      "   macro avg       0.52      0.58      0.45       601\n",
      "weighted avg       0.90      0.62      0.72       601\n",
      "\n",
      "\n",
      "--- Prefix 23 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.55      0.69       436\n",
      "           1       0.13      0.65      0.22        46\n",
      "\n",
      "    accuracy                           0.56       482\n",
      "   macro avg       0.53      0.60      0.45       482\n",
      "weighted avg       0.86      0.56      0.64       482\n",
      "\n",
      "\n",
      "--- Prefix 24 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.57      0.71       371\n",
      "           1       0.09      0.55      0.16        29\n",
      "\n",
      "    accuracy                           0.57       400\n",
      "   macro avg       0.52      0.56      0.43       400\n",
      "weighted avg       0.88      0.57      0.67       400\n",
      "\n",
      "\n",
      "--- Prefix 25 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.47      0.62       291\n",
      "           1       0.09      0.60      0.15        25\n",
      "\n",
      "    accuracy                           0.48       316\n",
      "   macro avg       0.51      0.53      0.39       316\n",
      "weighted avg       0.86      0.48      0.59       316\n",
      "\n",
      "\n",
      "--- Prefix 26 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.43      0.60       228\n",
      "           1       0.12      0.90      0.22        20\n",
      "\n",
      "    accuracy                           0.47       248\n",
      "   macro avg       0.55      0.67      0.41       248\n",
      "weighted avg       0.91      0.47      0.57       248\n",
      "\n",
      "\n",
      "--- Prefix 27 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.38      0.55       185\n",
      "           1       0.12      0.83      0.20        18\n",
      "\n",
      "    accuracy                           0.42       203\n",
      "   macro avg       0.54      0.61      0.38       203\n",
      "weighted avg       0.88      0.42      0.52       203\n",
      "\n",
      "\n",
      "--- Prefix 28 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.28      0.43       148\n",
      "           1       0.09      0.85      0.17        13\n",
      "\n",
      "    accuracy                           0.32       161\n",
      "   macro avg       0.52      0.56      0.30       161\n",
      "weighted avg       0.88      0.32      0.41       161\n",
      "\n",
      "\n",
      "--- Prefix 29 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.24      0.39       120\n",
      "           1       0.09      0.90      0.16        10\n",
      "\n",
      "    accuracy                           0.29       130\n",
      "   macro avg       0.53      0.57      0.28       130\n",
      "weighted avg       0.90      0.29      0.37       130\n",
      "\n",
      "\n",
      "--- Prefix 30 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.26      0.41        89\n",
      "           1       0.12      0.90      0.21        10\n",
      "\n",
      "    accuracy                           0.32        99\n",
      "   macro avg       0.54      0.58      0.31        99\n",
      "weighted avg       0.87      0.32      0.39        99\n",
      "\n",
      "\n",
      "--- Prefix 31 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.16      0.27        68\n",
      "           1       0.11      0.70      0.19        10\n",
      "\n",
      "    accuracy                           0.23        78\n",
      "   macro avg       0.45      0.43      0.23        78\n",
      "weighted avg       0.70      0.23      0.26        78\n",
      "\n",
      "\n",
      "--- Prefix 32 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.24        65\n",
      "           1       0.03      1.00      0.07         2\n",
      "\n",
      "    accuracy                           0.16        67\n",
      "   macro avg       0.52      0.57      0.15        67\n",
      "weighted avg       0.97      0.16      0.24        67\n",
      "\n",
      "\n",
      "--- Prefix 33 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.16      0.27        51\n",
      "           1       0.12      0.86      0.21         7\n",
      "\n",
      "    accuracy                           0.24        58\n",
      "   macro avg       0.51      0.51      0.24        58\n",
      "weighted avg       0.80      0.24      0.26        58\n",
      "\n",
      "\n",
      "--- Prefix 34 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.24        43\n",
      "           1       0.05      1.00      0.10         2\n",
      "\n",
      "    accuracy                           0.18        45\n",
      "   macro avg       0.53      0.57      0.17        45\n",
      "weighted avg       0.96      0.18      0.24        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) Summarize\n",
    "reports_df = pd.DataFrame(online_reports)\n",
    "for p in prefix_range:\n",
    "    sub = reports_df[reports_df[\"prefix\"] == p]\n",
    "    if not sub.empty:\n",
    "        print(f\"\\n--- Prefix {p} ---\")\n",
    "        print(classification_report(\n",
    "            sub[\"true_noise\"], sub[\"pred_noise\"], zero_division=0\n",
    "        ))\n",
    "reports_df\n",
    "reports_df.to_csv('../result/%s_fixed_v2.csv'%(dataset), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7313e-a771-4d3b-9ee1-fd5a9da854d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
