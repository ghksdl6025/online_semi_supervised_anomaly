{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e8f5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, log_loss, roc_auc_score, make_scorer\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b36662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform_target(encoder, targets, unknown_value=-1):\n",
    "    classes = set(encoder.classes_)\n",
    "    transformed = []\n",
    "    for t in targets:\n",
    "        if t in classes:\n",
    "            transformed.append(encoder.transform([t])[0])\n",
    "        else:\n",
    "            transformed.append(unknown_value)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432ffdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cls_result(classification_result):\n",
    "    \n",
    "    for i in classification_result.keys():\n",
    "        print(i, classification_result[i].keys())\n",
    "\n",
    "        if '1' not in classification_result[i].keys():\n",
    "            classification_result[i]['1'] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0.0}\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711c6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Read and Process the Data\n",
    "# ----------------------------\n",
    "dataset = '0.099_sample.csv'\n",
    "df = pd.read_csv(\"../data/%s\" % (dataset))\n",
    "df = df.sort_values(by='Timestamp')\n",
    "# Process the 'noise' column:\n",
    "# - If NaN, assume Normal (0).\n",
    "# - Otherwise, treat True/1/'True' as anomaly (1); everything else as Normal (0).\n",
    "df['noise'] = df['noise'].fillna(0).apply(lambda x: 1 if (x == True or x == 1 or x == 'True' or x=='true') else 0)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(dataset)\n",
    "# Calculate the cutoff time (e.g., the median of all timestamps)\n",
    "cutoff_time = df['Timestamp'].median()\n",
    "\n",
    "anomaly_f1_list = []\n",
    "anomaly_support_list = []\n",
    "prefix_range = range(2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4055886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training window size: 0.8\n",
      "Total cases with at least 2 events: 500\n",
      "Encoded feature shape: (500, 34)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 3 events: 500\n",
      "Encoded feature shape: (500, 52)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 4 events: 500\n",
      "Encoded feature shape: (500, 70)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 5 events: 500\n",
      "Encoded feature shape: (500, 88)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 6 events: 500\n",
      "Encoded feature shape: (500, 105)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 7 events: 500\n",
      "Encoded feature shape: (500, 123)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 8 events: 500\n",
      "Encoded feature shape: (500, 141)\n",
      "Training cases: 400 Test cases: 100\n",
      "Total cases with at least 9 events: 446\n",
      "Encoded feature shape: (446, 159)\n",
      "Training cases: 356 Test cases: 90\n",
      "Total cases with at least 10 events: 402\n",
      "Encoded feature shape: (402, 176)\n",
      "Training cases: 321 Test cases: 81\n",
      "Total cases with at least 11 events: 373\n",
      "Encoded feature shape: (373, 189)\n",
      "Training cases: 298 Test cases: 75\n",
      "Total cases with at least 12 events: 318\n",
      "Encoded feature shape: (318, 205)\n",
      "Training cases: 254 Test cases: 64\n",
      "Total cases with at least 13 events: 241\n",
      "Encoded feature shape: (241, 217)\n",
      "Training cases: 192 Test cases: 49\n",
      "Total cases with at least 14 events: 201\n",
      "Encoded feature shape: (201, 221)\n",
      "Training cases: 160 Test cases: 41\n",
      "Total cases with at least 15 events: 171\n",
      "Encoded feature shape: (171, 224)\n",
      "Training cases: 136 Test cases: 35\n",
      "2 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "3 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "4 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "5 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "6 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "7 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "8 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "9 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "10 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "11 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "12 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "13 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "14 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "15 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefix length</th>\n",
       "      <th>Normal precision</th>\n",
       "      <th>Normal recall</th>\n",
       "      <th>Normal f1-score</th>\n",
       "      <th>Normal support</th>\n",
       "      <th>Anomal precision</th>\n",
       "      <th>Anomal recall</th>\n",
       "      <th>Anomal f1-score</th>\n",
       "      <th>Anomal support</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Macro f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.774892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.801075</td>\n",
       "      <td>0.513094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.469269</td>\n",
       "      <td>0.454823</td>\n",
       "      <td>0.187970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.577997</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.532225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.504202</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.563581</td>\n",
       "      <td>0.628788</td>\n",
       "      <td>0.387903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.534903</td>\n",
       "      <td>0.576039</td>\n",
       "      <td>0.435778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.511220</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.313694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.493320</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.405048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.654867</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.523476</td>\n",
       "      <td>0.565925</td>\n",
       "      <td>0.429474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.474206</td>\n",
       "      <td>0.411864</td>\n",
       "      <td>0.336137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.431402</td>\n",
       "      <td>0.396780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.453333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.205214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prefix length  Normal precision  Normal recall  Normal f1-score  \\\n",
       "0               2          1.000000       0.853933         0.921212   \n",
       "1               3          1.000000       0.602151         0.751678   \n",
       "2               4          0.857143       0.131868         0.228571   \n",
       "3               5          0.965517       0.622222         0.756757   \n",
       "4               6          0.967742       0.340909         0.504202   \n",
       "5               7          0.909091       0.459770         0.610687   \n",
       "6               8          0.935484       0.315217         0.471545   \n",
       "7               9          0.893617       0.518519         0.656250   \n",
       "8              10          0.925000       0.506849         0.654867   \n",
       "9              11          0.933333       0.400000         0.560000   \n",
       "10             12          0.892857       0.423729         0.574713   \n",
       "11             13          0.800000       0.487805         0.606061   \n",
       "12             14          1.000000       0.500000         0.666667   \n",
       "13             15          1.000000       0.205882         0.341463   \n",
       "\n",
       "    Normal support  Anomal precision  Anomal recall  Anomal f1-score  \\\n",
       "0             89.0          0.458333       1.000000         0.628571   \n",
       "1             93.0          0.159091       1.000000         0.274510   \n",
       "2             91.0          0.081395       0.777778         0.147368   \n",
       "3             90.0          0.190476       0.800000         0.307692   \n",
       "4             88.0          0.159420       0.916667         0.271605   \n",
       "5             87.0          0.160714       0.692308         0.260870   \n",
       "6             92.0          0.086957       0.750000         0.155844   \n",
       "7             81.0          0.093023       0.444444         0.153846   \n",
       "8             73.0          0.121951       0.625000         0.204082   \n",
       "9             70.0          0.066667       0.600000         0.120000   \n",
       "10            59.0          0.055556       0.400000         0.097561   \n",
       "11            41.0          0.125000       0.375000         0.187500   \n",
       "12            38.0          0.136364       1.000000         0.240000   \n",
       "13            34.0          0.035714       1.000000         0.068966   \n",
       "\n",
       "    Anomal support  Macro precision  Macro recall  Macro f1-score  \n",
       "0             11.0         0.729167      0.926966        0.774892  \n",
       "1              7.0         0.579545      0.801075        0.513094  \n",
       "2              9.0         0.469269      0.454823        0.187970  \n",
       "3             10.0         0.577997      0.711111        0.532225  \n",
       "4             12.0         0.563581      0.628788        0.387903  \n",
       "5             13.0         0.534903      0.576039        0.435778  \n",
       "6              8.0         0.511220      0.532609        0.313694  \n",
       "7              9.0         0.493320      0.481481        0.405048  \n",
       "8              8.0         0.523476      0.565925        0.429474  \n",
       "9              5.0         0.500000      0.500000        0.340000  \n",
       "10             5.0         0.474206      0.411864        0.336137  \n",
       "11             8.0         0.462500      0.431402        0.396780  \n",
       "12             3.0         0.568182      0.750000        0.453333  \n",
       "13             1.0         0.517857      0.602941        0.205214  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = 0.8\n",
    "print('Training window size: %s' % (training_size))\n",
    "loss_prefix_dict =dict()\n",
    "classification_result = dict()\n",
    "anomaly_thr_method = 'diff'\n",
    "adaptive_thr_dict = dict()\n",
    "tuned_parameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': ['auto', 128, 256],\n",
    "    'contamination': [0.05, 0.1, 0.15],\n",
    "    'max_features': [1.0]\n",
    "}\n",
    "\n",
    "for prefix in prefix_range:    \n",
    "    # Extract per case:\n",
    "    # - The first (prefix-1) events (activities) as features.\n",
    "    # - The prefix-th event's activity as the target.\n",
    "    # - The prefix-th event's noise flag as the ground truth anomaly.\n",
    "    case_features = []\n",
    "    case_targets = []\n",
    "    ground_truth_anomaly = []\n",
    "\n",
    "    for case_id, group in df.groupby('ID'):\n",
    "        group = group.sort_index()  # assuming the order in the file is the event order\n",
    "        if len(group) >= prefix:\n",
    "            events = group['Activity'].values  # adjust 'Activity' if needed\n",
    "            features = events[:prefix]\n",
    "            noise_flag = group['noise'].iloc[prefix-1]\n",
    "\n",
    "            case_features.append(features)\n",
    "            ground_truth_anomaly.append(noise_flag)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    case_features = np.array(case_features)\n",
    "    ground_truth_anomaly = np.array(ground_truth_anomaly)\n",
    "    print(\"Total cases with at least %s events:\" % (prefix), case_features.shape[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Encode the Features and Target\n",
    "    # ----------------------------\n",
    "    encoder_features = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    X_encoded = encoder_features.fit_transform(case_features)\n",
    "    print(\"Encoded feature shape:\", X_encoded.shape)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Ordered Train/Test Split and Next Event Prediction Model (Stage 1)\n",
    "    # ----------------------------\n",
    "    # Instead of a random split, take the first 80% for training and the remaining 20% for testing.\n",
    "    n_cases = X_encoded.shape[0]\n",
    "    split_index = int(training_size * n_cases)\n",
    "    test_index = split_index\n",
    "    X_train = X_encoded[:split_index]\n",
    "    X_test = X_encoded[test_index:]\n",
    "    gt_anomaly_train = ground_truth_anomaly[:split_index]\n",
    "    gt_anomaly_test = ground_truth_anomaly[test_index:]\n",
    "    print(\"Training cases:\", X_train.shape[0], \"Test cases:\", X_test.shape[0])\n",
    "    train_feature_df = pd.DataFrame(X_train)   \n",
    "    test_feature_df = pd.DataFrame(X_test)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 3: Train model\n",
    "    # ----------------------------\n",
    "\n",
    "    # Train Isolation Forest with the training set. \n",
    "    anom_clf = OneClassSVM(gamma='auto')\n",
    "    \n",
    "    # Fit on training data\n",
    "    anom_clf.fit(train_feature_df)\n",
    "    # ----------------------------\n",
    "    # Step 4: Anomaly Detection (Stage 2) with Isolation Forest\n",
    "    # ----------------------------\n",
    "    \n",
    "    # Predict on test set\n",
    "    predictions = anom_clf.predict(test_feature_df)\n",
    "    # Convert to binary anomaly labels (1 for anomaly, 0 for normal)\n",
    "    binary_preds = np.where(predictions == -1, 1, 0)\n",
    "\n",
    "    test_feature_df['anomaly'] = binary_preds\n",
    "\n",
    "    # Evaluate if you have true labels for test set\n",
    "\n",
    "    # Calculate anomaly scores and classify anomalies\n",
    "    predicted_anomaly = test_feature_df['anomaly']\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 5: Evaluate the Anomaly Detection\n",
    "    # ----------------------------\n",
    "\n",
    "    classification = classification_report(gt_anomaly_test, predicted_anomaly, output_dict=True)\n",
    "    f1 = classification.get('1', {}).get('f1-score', 0)\n",
    "    support = classification.get('1', {}).get('support', 0)\n",
    "    classification_result[prefix] = classification\n",
    "#     classification_result[prefix]['ROC AUC'] = roc_auc_score(gt_anomaly_test, anom_clf.predict_proba(x_detect_test)[:,1])\n",
    "\n",
    "classification_result = cleaning_cls_result(classification_result)\n",
    "revised_cls_result = {}\n",
    "for i in classification_result.keys():\n",
    "    revised_cls_result[i] = dict()\n",
    "    revised_cls_result[i]['Normal precision'] =classification_result[i]['0']['precision']\n",
    "    revised_cls_result[i]['Normal recall'] =classification_result[i]['0']['recall']\n",
    "    revised_cls_result[i]['Normal f1-score'] =classification_result[i]['0']['f1-score']\n",
    "    revised_cls_result[i]['Normal support'] =classification_result[i]['0']['support']\n",
    "\n",
    "    revised_cls_result[i]['Anomal precision'] =classification_result[i]['1']['precision']\n",
    "    revised_cls_result[i]['Anomal recall'] =classification_result[i]['1']['recall']\n",
    "    revised_cls_result[i]['Anomal f1-score'] =classification_result[i]['1']['f1-score']\n",
    "    revised_cls_result[i]['Anomal support'] =classification_result[i]['1']['support']    \n",
    "\n",
    "    revised_cls_result[i]['Macro precision'] =classification_result[i]['macro avg']['precision']   \n",
    "    revised_cls_result[i]['Macro recall'] =classification_result[i]['macro avg']['recall']   \n",
    "    revised_cls_result[i]['Macro f1-score'] =classification_result[i]['macro avg']['f1-score']   \n",
    "#     revised_cls_result[i]['ROC AUC'] =classification_result[i]['ROC AUC']   \n",
    "result_df = pd.DataFrame.from_dict(revised_cls_result).T\n",
    "result_df.index = result_df.index.set_names(['Prefix length'])\n",
    "result_df = result_df.reset_index(drop=False)\n",
    "# result_file_title = '../result/%s_cross_entropy_%sfold_%s_anomal_thr_result.csv'%(dataset, fold, anomaly_thr_method)\n",
    "# print(result_file_title)\n",
    "result_df\n",
    "# result_df.to_csv(result_file_title, index=False)\n",
    "\n",
    "# loss_prefix_title = '../result/%s_cross_entropy_loss_list.json'%(dataset)\n",
    "# with open(loss_prefix_title, 'w') as f:\n",
    "#     json.dump(adaptive_thr_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63997e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
