{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c7fbd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, log_loss, roc_auc_score, make_scorer\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3438407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform_target(encoder, targets, unknown_value=-1):\n",
    "    classes = set(encoder.classes_)\n",
    "    transformed = []\n",
    "    for t in targets:\n",
    "        if t in classes:\n",
    "            transformed.append(encoder.transform([t])[0])\n",
    "        else:\n",
    "            transformed.append(unknown_value)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f9c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_loss(normal_loss_value, cross_entropy_loss_value):\n",
    "    normal_loss_dist = []\n",
    "    cross_loss_dist = []\n",
    "    for pos, prediction in  enumerate(normal_loss_value):\n",
    "        if prediction != 1:\n",
    "            cross_loss_dist.append(cross_entropy_loss_value[pos])\n",
    "            normal_loss_dist.append(prediction)\n",
    "\n",
    "    return normal_loss_dist, cross_loss_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e04d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cls_result(classification_result):\n",
    "    \n",
    "    for i in classification_result.keys():\n",
    "        print(i, classification_result[i].keys())\n",
    "\n",
    "        if '1' not in classification_result[i].keys():\n",
    "            classification_result[i]['1'] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0.0}\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddc7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Read and Process the Data\n",
    "# ----------------------------\n",
    "dataset = '0.099_sample.csv'\n",
    "df = pd.read_csv(\"../data/%s\" % (dataset))\n",
    "df = df.sort_values(by='Timestamp')\n",
    "# Process the 'noise' column:\n",
    "# - If NaN, assume Normal (0).\n",
    "# - Otherwise, treat True/1/'True' as anomaly (1); everything else as Normal (0).\n",
    "df['noise'] = df['noise'].fillna(0).apply(lambda x: 1 if (x == True or x == 1 or x == 'True' or x=='true') else 0)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(dataset)\n",
    "# Calculate the cutoff time (e.g., the median of all timestamps)\n",
    "cutoff_time = df['Timestamp'].median()\n",
    "\n",
    "anomaly_f1_list = []\n",
    "anomaly_support_list = []\n",
    "prefix_range = range(2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e43f570",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training window size: 0.8\n",
      "Total cases with at least 2 events: 500\n",
      "Encoded feature shape: (500, 34)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.105\n",
      "Total cases with at least 3 events: 500\n",
      "Encoded feature shape: (500, 52)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.518\n",
      "Total cases with at least 4 events: 500\n",
      "Encoded feature shape: (500, 70)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.304\n",
      "Total cases with at least 5 events: 500\n",
      "Encoded feature shape: (500, 88)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.289\n",
      "Total cases with at least 6 events: 500\n",
      "Encoded feature shape: (500, 105)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.565\n",
      "Total cases with at least 7 events: 500\n",
      "Encoded feature shape: (500, 123)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.257\n",
      "Total cases with at least 8 events: 500\n",
      "Encoded feature shape: (500, 141)\n",
      "Training cases: 400 Test cases: 100\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.365\n",
      "Total cases with at least 9 events: 446\n",
      "Encoded feature shape: (446, 159)\n",
      "Training cases: 356 Test cases: 90\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.619\n",
      "Total cases with at least 10 events: 402\n",
      "Encoded feature shape: (402, 176)\n",
      "Training cases: 321 Test cases: 81\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.366\n",
      "Total cases with at least 11 events: 373\n",
      "Encoded feature shape: (373, 189)\n",
      "Training cases: 298 Test cases: 75\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.571\n",
      "Total cases with at least 12 events: 318\n",
      "Encoded feature shape: (318, 205)\n",
      "Training cases: 254 Test cases: 64\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.529\n",
      "Total cases with at least 13 events: 241\n",
      "Encoded feature shape: (241, 217)\n",
      "Training cases: 192 Test cases: 49\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.220\n",
      "Total cases with at least 14 events: 201\n",
      "Encoded feature shape: (201, 221)\n",
      "Training cases: 160 Test cases: 41\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.202\n",
      "Total cases with at least 15 events: 171\n",
      "Encoded feature shape: (171, 224)\n",
      "Training cases: 136 Test cases: 35\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'contamination': 0.25, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 50}\n",
      "Best CV ROC AUC: nan\n",
      "Test ROC AUC: 0.588\n",
      "2 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "3 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "4 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "5 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "6 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "7 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "8 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "9 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "10 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "11 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "12 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "13 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "14 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "15 dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefix length</th>\n",
       "      <th>Normal precision</th>\n",
       "      <th>Normal recall</th>\n",
       "      <th>Normal f1-score</th>\n",
       "      <th>Normal support</th>\n",
       "      <th>Anomal precision</th>\n",
       "      <th>Anomal recall</th>\n",
       "      <th>Anomal f1-score</th>\n",
       "      <th>Anomal support</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Macro f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.774892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.524578</td>\n",
       "      <td>0.484634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.473177</td>\n",
       "      <td>0.445665</td>\n",
       "      <td>0.450980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.519763</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>0.510071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.824242</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.506776</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.497835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.568087</td>\n",
       "      <td>0.615827</td>\n",
       "      <td>0.569995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.557018</td>\n",
       "      <td>0.641304</td>\n",
       "      <td>0.553571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.475309</td>\n",
       "      <td>0.461967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.534016</td>\n",
       "      <td>0.571062</td>\n",
       "      <td>0.525053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.537076</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.529347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.503205</td>\n",
       "      <td>0.506780</td>\n",
       "      <td>0.491256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.576220</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.687976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prefix length  Normal precision  Normal recall  Normal f1-score  \\\n",
       "0               2          1.000000       0.853933         0.921212   \n",
       "1               3          0.934211       0.763441         0.840237   \n",
       "2               4          0.898734       0.780220         0.835294   \n",
       "3               5          0.909091       0.777778         0.838323   \n",
       "4               6          0.883117       0.772727         0.824242   \n",
       "5               7          0.905405       0.770115         0.832298   \n",
       "6               8          0.947368       0.782609         0.857143   \n",
       "7               9          0.893939       0.728395         0.802721   \n",
       "8              10          0.918033       0.767123         0.835821   \n",
       "9              11          0.949153       0.800000         0.868217   \n",
       "10             12          0.923077       0.813559         0.864865   \n",
       "11             13          0.860465       0.902439         0.880952   \n",
       "12             14          0.971429       0.894737         0.931507   \n",
       "13             15          0.966667       0.852941         0.906250   \n",
       "\n",
       "    Normal support  Anomal precision  Anomal recall  Anomal f1-score  \\\n",
       "0             89.0          0.458333       1.000000         0.628571   \n",
       "1             93.0          0.083333       0.285714         0.129032   \n",
       "2             91.0          0.047619       0.111111         0.066667   \n",
       "3             90.0          0.130435       0.300000         0.181818   \n",
       "4             88.0          0.130435       0.250000         0.171429   \n",
       "5             87.0          0.230769       0.461538         0.307692   \n",
       "6             92.0          0.166667       0.500000         0.250000   \n",
       "7             81.0          0.083333       0.222222         0.121212   \n",
       "8             73.0          0.150000       0.375000         0.214286   \n",
       "9             70.0          0.125000       0.400000         0.190476   \n",
       "10            59.0          0.083333       0.200000         0.117647   \n",
       "11            41.0          0.333333       0.250000         0.285714   \n",
       "12            38.0          0.333333       0.666667         0.444444   \n",
       "13            34.0          0.000000       0.000000         0.000000   \n",
       "\n",
       "    Anomal support  Macro precision  Macro recall  Macro f1-score  \n",
       "0             11.0         0.729167      0.926966        0.774892  \n",
       "1              7.0         0.508772      0.524578        0.484634  \n",
       "2              9.0         0.473177      0.445665        0.450980  \n",
       "3             10.0         0.519763      0.538889        0.510071  \n",
       "4             12.0         0.506776      0.511364        0.497835  \n",
       "5             13.0         0.568087      0.615827        0.569995  \n",
       "6              8.0         0.557018      0.641304        0.553571  \n",
       "7              9.0         0.488636      0.475309        0.461967  \n",
       "8              8.0         0.534016      0.571062        0.525053  \n",
       "9              5.0         0.537076      0.600000        0.529347  \n",
       "10             5.0         0.503205      0.506780        0.491256  \n",
       "11             8.0         0.596899      0.576220        0.583333  \n",
       "12             3.0         0.652381      0.780702        0.687976  \n",
       "13             1.0         0.483333      0.426471        0.453125  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = 0.8\n",
    "print('Training window size: %s' % (training_size))\n",
    "loss_prefix_dict =dict()\n",
    "classification_result = dict()\n",
    "anomaly_thr_method = 'diff'\n",
    "adaptive_thr_dict = dict()\n",
    "tuned_parameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': ['auto', 128, 256],\n",
    "    'contamination': [0.25],\n",
    "    'max_features': [1.0]\n",
    "}\n",
    "\n",
    "for prefix in prefix_range:    \n",
    "    # Extract per case:\n",
    "    # - The first (prefix-1) events (activities) as features.\n",
    "    # - The prefix-th event's activity as the target.\n",
    "    # - The prefix-th event's noise flag as the ground truth anomaly.\n",
    "    case_features = []\n",
    "    case_targets = []\n",
    "    ground_truth_anomaly = []\n",
    "\n",
    "    for case_id, group in df.groupby('ID'):\n",
    "        group = group.sort_index()  # assuming the order in the file is the event order\n",
    "        if len(group) >= prefix:\n",
    "            events = group['Activity'].values  # adjust 'Activity' if needed\n",
    "            features = events[:prefix]\n",
    "            noise_flag = group['noise'].iloc[prefix-1]\n",
    "\n",
    "            case_features.append(features)\n",
    "            case_targets.append(target_activity)\n",
    "            ground_truth_anomaly.append(noise_flag)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    case_features = np.array(case_features)\n",
    "    case_targets = np.array(case_targets)\n",
    "    ground_truth_anomaly = np.array(ground_truth_anomaly)\n",
    "    print(\"Total cases with at least %s events:\" % (prefix), case_features.shape[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Encode the Features and Target\n",
    "    # ----------------------------\n",
    "    encoder_features = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    X_encoded = encoder_features.fit_transform(case_features)\n",
    "    print(\"Encoded feature shape:\", X_encoded.shape)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Ordered Train/Test Split and Next Event Prediction Model (Stage 1)\n",
    "    # ----------------------------\n",
    "    # Instead of a random split, take the first 80% for training and the remaining 20% for testing.\n",
    "    n_cases = X_encoded.shape[0]\n",
    "    split_index = int(training_size * n_cases)\n",
    "    test_index = split_index\n",
    "    X_train = X_encoded[:split_index]\n",
    "    X_test = X_encoded[test_index:]\n",
    "    gt_anomaly_train = ground_truth_anomaly[:split_index]\n",
    "    gt_anomaly_test = ground_truth_anomaly[test_index:]\n",
    "    print(\"Training cases:\", X_train.shape[0], \"Test cases:\", X_test.shape[0])\n",
    "    train_feature_df = pd.DataFrame(X_train)   \n",
    "    test_feature_df = pd.DataFrame(X_test)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 3: Train model\n",
    "    # ----------------------------\n",
    "\n",
    "    # Train Isolation Forest with the training set. \n",
    "    anom_clf = IsolationForest(random_state=42)   \n",
    "    \n",
    "    # Use a stratified split to maintain class balance in folds\n",
    "    cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "    # Custom scorer: use ROC AUC on the anomaly scores\n",
    "    def anomaly_scorer(estimator, X, y):\n",
    "        # decision_function gives anomaly scores (higher = more normal)\n",
    "        scores = estimator.decision_function(X)\n",
    "        return roc_auc_score(y, scores)\n",
    "\n",
    "    scorer = make_scorer(anomaly_scorer, greater_is_better=True)\n",
    "\n",
    "    # Custom scorer: macro F1 on the binary predictions\n",
    "    def iso_anomaly_f1_scorer(estimator, X, y_true):\n",
    "        # Predict returns +1 for inliers, -1 for outliers\n",
    "        preds = estimator.predict(X)\n",
    "        # map to binary: 0=normal, 1=anomaly\n",
    "        y_pred = np.where(preds == -1, 1, 0)\n",
    "        # compute binary F1 for anomaly class (pos_label=1)\n",
    "        return f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    scorer = make_scorer(iso_f1_scorer, greater_is_better=True)    \n",
    "    \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=anom_clf,\n",
    "        param_grid=tuned_parameters,\n",
    "        scoring=scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    grid_search.fit(train_feature_df, gt_anomaly_train)\n",
    "\n",
    "    print(\"Best parameters found:\", grid_search.best_params_)\n",
    "    print(\"Best CV ROC AUC:\", grid_search.best_score_)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Anomaly Detection (Stage 2) with Isolation Forest\n",
    "    # ----------------------------\n",
    "    \n",
    "    # Use the best estimator for predictions\n",
    "    best_iso = grid_search.best_estimator_\n",
    "\n",
    "    # Predict on test set\n",
    "    test_scores = best_iso.decision_function(test_feature_df)\n",
    "    predictions = best_iso.predict(test_feature_df)\n",
    "    # Convert to binary anomaly labels (1 for anomaly, 0 for normal)\n",
    "    binary_preds = np.where(predictions == -1, 1, 0)\n",
    "\n",
    "    test_feature_df['anomaly_score'] = test_scores\n",
    "    test_feature_df['anomaly'] = binary_preds\n",
    "\n",
    "    # Evaluate if you have true labels for test set\n",
    "    test_auc = roc_auc_score(gt_anomaly_test, test_scores)\n",
    "    print(f\"Test ROC AUC: {test_auc:.3f}\")\n",
    "\n",
    "    # Calculate anomaly scores and classify anomalies\n",
    "    predicted_anomaly = test_feature_df['anomaly']\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 5: Evaluate the Anomaly Detection\n",
    "    # ----------------------------\n",
    "\n",
    "    classification = classification_report(gt_anomaly_test, predicted_anomaly, output_dict=True)\n",
    "    f1 = classification.get('1', {}).get('f1-score', 0)\n",
    "    support = classification.get('1', {}).get('support', 0)\n",
    "    classification_result[prefix] = classification\n",
    "#     classification_result[prefix]['ROC AUC'] = roc_auc_score(gt_anomaly_test, anom_clf.predict_proba(x_detect_test)[:,1])\n",
    "\n",
    "classification_result = cleaning_cls_result(classification_result)\n",
    "revised_cls_result = {}\n",
    "for i in classification_result.keys():\n",
    "    revised_cls_result[i] = dict()\n",
    "    revised_cls_result[i]['Normal precision'] =classification_result[i]['0']['precision']\n",
    "    revised_cls_result[i]['Normal recall'] =classification_result[i]['0']['recall']\n",
    "    revised_cls_result[i]['Normal f1-score'] =classification_result[i]['0']['f1-score']\n",
    "    revised_cls_result[i]['Normal support'] =classification_result[i]['0']['support']\n",
    "\n",
    "    revised_cls_result[i]['Anomal precision'] =classification_result[i]['1']['precision']\n",
    "    revised_cls_result[i]['Anomal recall'] =classification_result[i]['1']['recall']\n",
    "    revised_cls_result[i]['Anomal f1-score'] =classification_result[i]['1']['f1-score']\n",
    "    revised_cls_result[i]['Anomal support'] =classification_result[i]['1']['support']    \n",
    "\n",
    "    revised_cls_result[i]['Macro precision'] =classification_result[i]['macro avg']['precision']   \n",
    "    revised_cls_result[i]['Macro recall'] =classification_result[i]['macro avg']['recall']   \n",
    "    revised_cls_result[i]['Macro f1-score'] =classification_result[i]['macro avg']['f1-score']   \n",
    "#     revised_cls_result[i]['ROC AUC'] =classification_result[i]['ROC AUC']   \n",
    "result_df = pd.DataFrame.from_dict(revised_cls_result).T\n",
    "result_df.index = result_df.index.set_names(['Prefix length'])\n",
    "result_df = result_df.reset_index(drop=False)\n",
    "# result_file_title = '../result/%s_cross_entropy_%sfold_%s_anomal_thr_result.csv'%(dataset, fold, anomaly_thr_method)\n",
    "# print(result_file_title)\n",
    "result_df\n",
    "# result_df.to_csv(result_file_title, index=False)\n",
    "\n",
    "# loss_prefix_title = '../result/%s_cross_entropy_loss_list.json'%(dataset)\n",
    "# with open(loss_prefix_title, 'w') as f:\n",
    "#     json.dump(adaptive_thr_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4591fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
