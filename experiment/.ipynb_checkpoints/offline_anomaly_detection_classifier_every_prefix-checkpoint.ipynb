{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53efb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import datetime\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, log_loss, roc_auc_score\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b11ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, x_test, y_test):\n",
    "    \n",
    "    probs = model.predict_proba(x_test)\n",
    "    \n",
    "    predicted_probs = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if len(idx_arr) == 0:\n",
    "            predicted_probs.append(log_loss(y_true = [1,0], y_pred=[0,1])+1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            true_label_one_hot = np.zeros_like(probs[i])\n",
    "            true_label_one_hot[idx_arr] = 1\n",
    "            predicted_probs.append(log_loss(y_true = true_label_one_hot, y_pred = probs[i]))\n",
    "            \n",
    "    return np.array(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342de8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_loss(model, x_test, y_test):\n",
    "    \n",
    "    probs = model.predict_proba(x_test)\n",
    "\n",
    "    predicted_probs = []\n",
    "    for i, true_label in enumerate(y_test):\n",
    "        idx_arr = np.where(model.classes_ == true_label)[0]\n",
    "        if len(idx_arr) == 0:\n",
    "            predicted_probs.append(1.1)\n",
    "        else:\n",
    "            col_index = idx_arr[0]\n",
    "            \n",
    "            true_label_one_hot = np.zeros_like(probs[i])\n",
    "            true_label_one_hot[idx_arr] = 1\n",
    "            predicted_probs.append(1-probs[i][col_index])\n",
    "            \n",
    "    return np.array(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e3c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform_target(encoder, targets, unknown_value=-1):\n",
    "    classes = set(encoder.classes_)\n",
    "    transformed = []\n",
    "    for t in targets:\n",
    "        if t in classes:\n",
    "            transformed.append(encoder.transform([t])[0])\n",
    "        else:\n",
    "            transformed.append(unknown_value)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f15a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_loss(normal_loss_value, cross_entropy_loss_value):\n",
    "    normal_loss_dist = []\n",
    "    cross_loss_dist = []\n",
    "    for pos, prediction in  enumerate(normal_loss_value):\n",
    "        if prediction != 1:\n",
    "            cross_loss_dist.append(cross_entropy_loss_value[pos])\n",
    "            normal_loss_dist.append(prediction)\n",
    "\n",
    "    return normal_loss_dist, cross_loss_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3591dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_cls_result(classification_result):\n",
    "    \n",
    "    for i in classification_result.keys():\n",
    "        print(i, classification_result[i].keys())\n",
    "\n",
    "        if '1' not in classification_result[i].keys():\n",
    "            classification_result[i]['1'] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0.0}\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5802d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_min_anomalies(gt_labels, num_samples=10, min_anomalies=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly sample `num_samples` indices from gt_labels (0/1 array),\n",
    "    ensuring at least `min_anomalies` true-anomaly (1) indices are included.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_labels : array-like, shape (n_samples,)\n",
    "        Ground-truth labels (0 = normal, 1 = anomaly).\n",
    "    num_samples : int, default=10\n",
    "        Total number of indices to sample.\n",
    "    min_anomalies : int, default=3\n",
    "        Minimum number of anomaly indices to include.\n",
    "    random_state : int or None\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    selected_indices : ndarray, shape (<= num_samples,)\n",
    "        Shuffled indices, containing at least `min_anomalies` anomalies\n",
    "        (or as many as available if fewer exist).\n",
    "    \"\"\"\n",
    "    gt_labels = np.asarray(gt_labels)\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # locate anomaly vs normal indices\n",
    "    anomaly_idx = np.where(gt_labels == 1)[0]\n",
    "    normal_idx  = np.where(gt_labels == 0)[0]\n",
    "\n",
    "    # determine how many anomalies we can pick\n",
    "    n_anom = min(len(anomaly_idx), min_anomalies)\n",
    "    # pick anomalies without replacement\n",
    "    picked_anom = np.random.choice(anomaly_idx, n_anom, replace=False) if n_anom > 0 else np.array([], dtype=int)\n",
    "\n",
    "    # fill the rest from normals\n",
    "    n_normal = num_samples - n_anom\n",
    "    n_normal = min(n_normal, len(normal_idx))\n",
    "    picked_norm = np.random.choice(normal_idx, n_normal, replace=False) if n_normal > 0 else np.array([], dtype=int)\n",
    "\n",
    "    # combine and shuffle\n",
    "    selected = np.concatenate([picked_anom, picked_norm])\n",
    "    np.random.shuffle(selected)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da9e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_ce_samples(ce_loss, y_true, n_samples=20, anomaly_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Select the samples with the highest cross-entropy loss, balanced between anomalies and normals.\n",
    "\n",
    "    Parameters:\n",
    "    - ce_loss: array-like of CE loss values\n",
    "    - y_true: array-like of binary labels (1=anomaly, 0=normal)\n",
    "    - n_samples: total number of samples to select\n",
    "    - anomaly_ratio: fraction of anomalies in the selected set (e.g., 0.5 for half anomalies)\n",
    "\n",
    "    Returns:\n",
    "    - selected_indices: numpy array of indices (into ce_loss / y_true) of the chosen samples\n",
    "    \"\"\"\n",
    "    ce_loss = np.asarray(ce_loss)\n",
    "    y_true = np.asarray(y_true)\n",
    "    n_anom = int(n_samples * anomaly_ratio)\n",
    "    n_norm = n_samples - n_anom\n",
    "\n",
    "    # Identify indices for each class\n",
    "    anom_idx = np.where(y_true == 1)[0]\n",
    "    norm_idx = np.where(y_true == 0)[0]\n",
    "\n",
    "    # Sort each group by descending loss and pick top-k\n",
    "    top_anom = anom_idx[np.argsort(-ce_loss[anom_idx])[:n_anom]]\n",
    "    top_norm = norm_idx[np.argsort(-ce_loss[norm_idx])[:n_norm]]\n",
    "\n",
    "    # Combine and return\n",
    "    selected_indices = np.concatenate([top_anom, top_norm])\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2dccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_gap(losses):\n",
    "    y = sorted(losses, reverse=True)\n",
    "    diffs = abs(np.diff(y))\n",
    "    idx = np.argmax(diffs) + 1   # +1 because diffs[i] = y[i+1]-y[i]\n",
    "    return idx, y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52694ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099_noise.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Read and Process the Data\n",
    "# ----------------------------\n",
    "dataset = '0.099_noise.csv'\n",
    "df = pd.read_csv(\"../data/%s\" % (dataset))\n",
    "df = df.sort_values(by='Timestamp')\n",
    "# Process the 'noise' column:\n",
    "# - If NaN, assume Normal (0).\n",
    "# - Otherwise, treat True/1/'True' as anomaly (1); everything else as Normal (0).\n",
    "df['noise'] = df['noise'].fillna(0).apply(lambda x: 1 if (x == True or x == 1 or x == 'True' or x=='true') else 0)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(dataset)\n",
    "# Calculate the cutoff time (e.g., the median of all timestamps)\n",
    "cutoff_time = df['Timestamp'].median()\n",
    "\n",
    "anomaly_f1_list = []\n",
    "anomaly_support_list = []\n",
    "prefix_range = range(2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faaa75a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training window size: 0.8\n",
      "Total cases with at least 2 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 1)\n",
      "Total cases with at least 3 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 19)\n",
      "Total cases with at least 4 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 37)\n",
      "Total cases with at least 5 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 55)\n",
      "Total cases with at least 6 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 73)\n",
      "Total cases with at least 7 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 91)\n",
      "Total cases with at least 8 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 109)\n",
      "Total cases with at least 9 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 127)\n",
      "Total cases with at least 10 events: 5000\n",
      "Training cases: 4000 Test cases: 1000\n",
      "Encoded feature shape: (5000, 145)\n",
      "Total cases with at least 11 events: 4522\n",
      "Training cases: 3617 Test cases: 905\n",
      "Encoded feature shape: (4522, 163)\n",
      "Total cases with at least 12 events: 4013\n",
      "Training cases: 3210 Test cases: 803\n",
      "Encoded feature shape: (4013, 181)\n",
      "Total cases with at least 13 events: 3794\n",
      "Training cases: 3035 Test cases: 759\n",
      "Encoded feature shape: (3794, 199)\n",
      "Total cases with at least 14 events: 3252\n",
      "Training cases: 2601 Test cases: 651\n",
      "Encoded feature shape: (3252, 217)\n",
      "Total cases with at least 15 events: 2548\n",
      "Training cases: 2038 Test cases: 510\n",
      "Encoded feature shape: (2548, 235)\n"
     ]
    }
   ],
   "source": [
    "training_size = 0.8\n",
    "print('Training window size: %s' % (training_size))\n",
    "loss_prefix_dict =dict()\n",
    "classification_result = dict()\n",
    "\n",
    "all_x_detect_train = []\n",
    "all_y_detect_train = []\n",
    "all_x_detect_test  = dict()\n",
    "all_y_detect_test  = dict()\n",
    "\n",
    "for prefix in prefix_range:    \n",
    "    # Extract per case:\n",
    "    # - The first (prefix-1) events (activities) as features.\n",
    "    # - The prefix-th event's activity as the target.\n",
    "    # - The prefix-th event's noise flag as the ground truth anomaly.\n",
    "    case_features = []\n",
    "    case_targets = []\n",
    "    ground_truth_anomaly = []\n",
    "\n",
    "    for case_id, group in df.groupby('Case ID'):\n",
    "        group = group.sort_index()  # assuming the order in the file is the event order\n",
    "        if len(group) >= prefix:\n",
    "            events = group['Activity'].values  # adjust 'Activity' if needed\n",
    "            features = events[:prefix-1]\n",
    "            target_activity = events[prefix-1]  # prefix-th event's activity\n",
    "            noise_flag = group['noise'].iloc[prefix-1]\n",
    "\n",
    "            case_features.append(features)\n",
    "            case_targets.append(target_activity)\n",
    "            ground_truth_anomaly.append(noise_flag)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    case_features = np.array(case_features)\n",
    "    case_targets = np.array(case_targets)\n",
    "    ground_truth_anomaly = np.array(ground_truth_anomaly)\n",
    "    print(\"Total cases with at least %s events:\" % (prefix), case_features.shape[0])\n",
    "    \n",
    "    n_cases = case_features.shape[0]\n",
    "    split_index = int(training_size * n_cases)\n",
    "    test_index = split_index\n",
    "    X_train = case_features[:split_index]\n",
    "    X_test = case_features[test_index:]\n",
    "    y_train = case_targets[:split_index]\n",
    "    y_test = case_targets[test_index:]\n",
    "    gt_anomaly_train = ground_truth_anomaly[:split_index]\n",
    "    gt_anomaly_test = ground_truth_anomaly[test_index:]\n",
    "    print(\"Training cases:\", X_train.shape[0], \"Test cases:\", X_test.shape[0])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Encode the Features and Target\n",
    "    # ----------------------------\n",
    "    encoder_features = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    \n",
    "    X_encoded = encoder_features.fit_transform(case_features)\n",
    "    print(\"Encoded feature shape:\", X_encoded.shape)\n",
    "\n",
    "    # IMPORTANT: Fit LabelEncoder on the full set of target activities (all cases)\n",
    "    target_encoder = LabelEncoder()\n",
    "    target_encoder.fit(case_targets)\n",
    "    y_encoded = target_encoder.transform(case_targets)\n",
    "    \n",
    "    # Train a RandomForest classifier with the training set.\n",
    "    nap_x_train = X_encoded[:split_index]\n",
    "    nap_y_train = y_encoded[:split_index]\n",
    "    nap_x_test = X_encoded[test_index:]\n",
    "    nap_y_test = y_encoded[test_index:]\n",
    "    rf_model  = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(nap_x_train, nap_y_train)\n",
    "\n",
    "    ce_loss = cross_entropy_loss(model=rf_model, x_test = nap_x_train, y_test = nap_y_train)       \n",
    "    idx, cutoff = find_largest_gap(ce_loss)\n",
    "    \n",
    "    predicted_anomaly = (ce_loss > cutoff).astype(int)\n",
    "    \n",
    "    expert_anomaly_indices = select_top_ce_samples(\n",
    "        ce_loss, gt_anomaly_train,\n",
    "        n_samples=20,\n",
    "        anomaly_ratio=0.5,\n",
    "    )\n",
    "    expert_anomaly_indices = sample_with_min_anomalies(\n",
    "            gt_labels=gt_anomaly_train,\n",
    "            num_samples=20,\n",
    "            min_anomalies=10,\n",
    "            random_state=42\n",
    "    )\n",
    "    \n",
    "    # Modify training set for anomaly detection classifier\n",
    "    x_detect_train = []\n",
    "    y_detect_train = []\n",
    "    for pos, idx in enumerate(expert_anomaly_indices):\n",
    "        new_training = dict()\n",
    "        new_training['activity_labels'] = X_train[idx]\n",
    "        new_training['target_labels'] = y_train[idx]\n",
    "        new_training['probability'] = rf_model.predict_proba([nap_x_train[idx]]).tolist()[0]\n",
    "        new_training['ce_loss'] = predicted_anomaly[idx]\n",
    "        \n",
    "        x_detect_train.append(new_training)\n",
    "        y_detect_train.append(gt_anomaly_train[idx])\n",
    "\n",
    "    all_x_detect_train.extend(x_detect_train)\n",
    "    all_y_detect_train.extend(y_detect_train)\n",
    "    \n",
    "    ce_loss = cross_entropy_loss(model=rf_model, x_test = nap_x_test, y_test = nap_y_test)\n",
    "    predicted_anomaly = (ce_loss > cutoff).astype(int)\n",
    "\n",
    "    # Modify test set for anomaly detection classifier\n",
    "    x_detect_test = []\n",
    "    y_detect_test = []\n",
    "    for idx in range(len(X_test)):\n",
    "        new_test = dict()\n",
    "        new_test['activity_labels'] = X_test[idx]\n",
    "        new_test['target_labels'] = y_test[idx]\n",
    "        new_test['probability'] = rf_model.predict_proba([nap_x_test[idx]]).tolist()[0]\n",
    "        new_test['ce_loss'] = predicted_anomaly[idx]\n",
    "        x_detect_test.append(new_test)\n",
    "        y_detect_test.append(gt_anomaly_test[idx])\n",
    "    all_x_detect_test[prefix]=x_detect_test\n",
    "    all_y_detect_test[prefix]=y_detect_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "514e4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 233)\n",
      "(280, 234)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=5,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=5,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=5,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_encoder_features = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "x_detect_train = pd.DataFrame()\n",
    "x_detect_train = detect_encoder_features.fit_transform(pd.DataFrame([i['activity_labels'] for i in all_x_detect_train]))\n",
    "x_detect_train = pd.DataFrame(x_detect_train)\n",
    "x_detect_train.columns = ['a%s'%(i) for i in x_detect_train.columns.values]\n",
    "\n",
    "detect_target_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "target_label = detect_target_encoder.fit_transform(pd.DataFrame([i['target_labels'] for i in all_x_detect_train], columns=['target_labels']))\n",
    "target_label = pd.DataFrame(target_label)\n",
    "x_detect_train = pd.concat([x_detect_train,target_label], axis=1)\n",
    "\n",
    "max_prob_length = max([len(i['probability']) for i in all_x_detect_train])\n",
    "for pos, i in enumerate(all_x_detect_train):\n",
    "    while len(i['probability']) < max_prob_length:\n",
    "        all_x_detect_train[pos]['probability'].append(0)\n",
    "x_detect_train = pd.concat([x_detect_train, pd.DataFrame([i['probability'] for i in all_x_detect_train], \n",
    "                           columns = ['p%s'%(i) for i in range(len(all_x_detect_train[0]['probability']))])], axis=1)\n",
    "print(x_detect_train.shape)\n",
    "x_detect_train = pd.concat([x_detect_train, pd.DataFrame([i['ce_loss'] for i in all_x_detect_train], columns=['ce_loss'])], axis=1)\n",
    "\n",
    "x_detect_train.columns = x_detect_train.columns.astype(str)\n",
    "print(x_detect_train.shape)\n",
    "# ----------------------------\n",
    "# Step 5: Making training set for the anomaly detection classifier \n",
    "# ----------------------------\n",
    "# anom_clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# anom_clf = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "anom_clf = XGBClassifier(objective='binary:logistic', n_estimators=5, learning_rate=0.01, eval_metric='logloss',\n",
    "                                         random_state=42)\n",
    "anom_clf.fit(x_detect_train, all_y_detect_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c64a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       140\n",
      "           1       0.88      0.80      0.84       140\n",
      "\n",
      "    accuracy                           0.85       280\n",
      "   macro avg       0.85      0.85      0.85       280\n",
      "weighted avg       0.85      0.85      0.85       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true= all_y_detect_train, y_pred = anom_clf.predict(x_detect_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77076135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 2 ---\n",
      "Classification Report: F1-score = 0.9852216748768473, Support = 103.0\n",
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 3 ---\n",
      "Classification Report: F1-score = 0.88, Support = 92.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 4 ---\n",
      "Classification Report: F1-score = 0.9473684210526315, Support = 102.0\n",
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 5 ---\n",
      "Classification Report: F1-score = 0.4595744680851064, Support = 111.0\n",
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 6 ---\n",
      "Classification Report: F1-score = 0.6442953020134228, Support = 99.0\n",
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 7 ---\n",
      "Classification Report: F1-score = 0.3277591973244147, Support = 116.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 8 ---\n",
      "Classification Report: F1-score = 0.2571428571428571, Support = 104.0\n",
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 9 ---\n",
      "Classification Report: F1-score = 0.3973509933774834, Support = 91.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 10 ---\n",
      "Classification Report: F1-score = 0.4110854503464203, Support = 93.0\n",
      "(905, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 11 ---\n",
      "Classification Report: F1-score = 0.27655310621242485, Support = 81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(803, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 12 ---\n",
      "Classification Report: F1-score = 0.2903885480572597, Support = 71.0\n",
      "(759, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 13 ---\n",
      "Classification Report: F1-score = 0.391644908616188, Support = 76.0\n",
      "(651, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 14 ---\n",
      "Classification Report: F1-score = 0.34972677595628415, Support = 73.0\n",
      "(510, 234)\n",
      "\n",
      "--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix 15 ---\n",
      "Classification Report: F1-score = 0.2939297124600639, Support = 46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Python\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefix length</th>\n",
       "      <th>Normal precision</th>\n",
       "      <th>Normal recall</th>\n",
       "      <th>Normal f1-score</th>\n",
       "      <th>Normal support</th>\n",
       "      <th>Anomal precision</th>\n",
       "      <th>Anomal recall</th>\n",
       "      <th>Anomal f1-score</th>\n",
       "      <th>Anomal support</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Macro f1-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998331</td>\n",
       "      <td>897.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970874</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.985437</td>\n",
       "      <td>0.991776</td>\n",
       "      <td>0.985437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.995516</td>\n",
       "      <td>0.977974</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>908.0</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.905165</td>\n",
       "      <td>0.967248</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.974263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.991091</td>\n",
       "      <td>0.993858</td>\n",
       "      <td>898.0</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.960937</td>\n",
       "      <td>0.980840</td>\n",
       "      <td>0.970613</td>\n",
       "      <td>0.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.995320</td>\n",
       "      <td>0.717660</td>\n",
       "      <td>0.833987</td>\n",
       "      <td>889.0</td>\n",
       "      <td>0.300836</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.459574</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.648078</td>\n",
       "      <td>0.845317</td>\n",
       "      <td>0.646781</td>\n",
       "      <td>0.981916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.996255</td>\n",
       "      <td>0.885683</td>\n",
       "      <td>0.937720</td>\n",
       "      <td>901.0</td>\n",
       "      <td>0.482412</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.644295</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.739333</td>\n",
       "      <td>0.927690</td>\n",
       "      <td>0.791008</td>\n",
       "      <td>0.983133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.965251</td>\n",
       "      <td>0.565611</td>\n",
       "      <td>0.713267</td>\n",
       "      <td>884.0</td>\n",
       "      <td>0.203320</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.327759</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.584285</td>\n",
       "      <td>0.705219</td>\n",
       "      <td>0.520513</td>\n",
       "      <td>0.785131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>896.0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.549536</td>\n",
       "      <td>0.631868</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.724964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.700770</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>909.0</td>\n",
       "      <td>0.248619</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.397351</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.623526</td>\n",
       "      <td>0.844891</td>\n",
       "      <td>0.610440</td>\n",
       "      <td>0.966676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.723264</td>\n",
       "      <td>0.837269</td>\n",
       "      <td>907.0</td>\n",
       "      <td>0.261765</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.411085</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.627852</td>\n",
       "      <td>0.840126</td>\n",
       "      <td>0.624177</td>\n",
       "      <td>0.965750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.975359</td>\n",
       "      <td>0.576456</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>824.0</td>\n",
       "      <td>0.165072</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.276553</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.570216</td>\n",
       "      <td>0.714154</td>\n",
       "      <td>0.500595</td>\n",
       "      <td>0.865801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525956</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>732.0</td>\n",
       "      <td>0.169856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.290389</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.584928</td>\n",
       "      <td>0.762978</td>\n",
       "      <td>0.489868</td>\n",
       "      <td>0.968964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.997788</td>\n",
       "      <td>0.660322</td>\n",
       "      <td>0.794714</td>\n",
       "      <td>683.0</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.391645</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.621044</td>\n",
       "      <td>0.823582</td>\n",
       "      <td>0.593179</td>\n",
       "      <td>0.967019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.974860</td>\n",
       "      <td>0.603806</td>\n",
       "      <td>0.745726</td>\n",
       "      <td>578.0</td>\n",
       "      <td>0.218430</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.349727</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.596645</td>\n",
       "      <td>0.740259</td>\n",
       "      <td>0.547727</td>\n",
       "      <td>0.884510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.523707</td>\n",
       "      <td>0.687412</td>\n",
       "      <td>464.0</td>\n",
       "      <td>0.172285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.293930</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.586142</td>\n",
       "      <td>0.761853</td>\n",
       "      <td>0.490671</td>\n",
       "      <td>0.946121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prefix length  Normal precision  Normal recall  Normal f1-score  \\\n",
       "0               2          0.996667       1.000000         0.998331   \n",
       "1               3          0.995516       0.977974         0.986667   \n",
       "2               4          0.996641       0.991091         0.993858   \n",
       "3               5          0.995320       0.717660         0.833987   \n",
       "4               6          0.996255       0.885683         0.937720   \n",
       "5               7          0.965251       0.565611         0.713267   \n",
       "6               8          0.941176       0.571429         0.711111   \n",
       "7               9          0.998433       0.700770         0.823529   \n",
       "8              10          0.993939       0.723264         0.837269   \n",
       "9              11          0.975359       0.576456         0.724638   \n",
       "10             12          1.000000       0.525956         0.689346   \n",
       "11             13          0.997788       0.660322         0.794714   \n",
       "12             14          0.974860       0.603806         0.745726   \n",
       "13             15          1.000000       0.523707         0.687412   \n",
       "\n",
       "    Normal support  Anomal precision  Anomal recall  Anomal f1-score  \\\n",
       "0            897.0          1.000000       0.970874         0.985222   \n",
       "1            908.0          0.814815       0.956522         0.880000   \n",
       "2            898.0          0.925234       0.970588         0.947368   \n",
       "3            889.0          0.300836       0.972973         0.459574   \n",
       "4            901.0          0.482412       0.969697         0.644295   \n",
       "5            884.0          0.203320       0.844828         0.327759   \n",
       "6            896.0          0.157895       0.692308         0.257143   \n",
       "7            909.0          0.248619       0.989011         0.397351   \n",
       "8            907.0          0.261765       0.956989         0.411085   \n",
       "9            824.0          0.165072       0.851852         0.276553   \n",
       "10           732.0          0.169856       1.000000         0.290389   \n",
       "11           683.0          0.244300       0.986842         0.391645   \n",
       "12           578.0          0.218430       0.876712         0.349727   \n",
       "13           464.0          0.172285       1.000000         0.293930   \n",
       "\n",
       "    Anomal support  Macro precision  Macro recall  Macro f1-score   ROC AUC  \n",
       "0            103.0         0.998333      0.985437        0.991776  0.985437  \n",
       "1             92.0         0.905165      0.967248        0.933333  0.974263  \n",
       "2            102.0         0.960937      0.980840        0.970613  0.990234  \n",
       "3            111.0         0.648078      0.845317        0.646781  0.981916  \n",
       "4             99.0         0.739333      0.927690        0.791008  0.983133  \n",
       "5            116.0         0.584285      0.705219        0.520513  0.785131  \n",
       "6            104.0         0.549536      0.631868        0.484127  0.724964  \n",
       "7             91.0         0.623526      0.844891        0.610440  0.966676  \n",
       "8             93.0         0.627852      0.840126        0.624177  0.965750  \n",
       "9             81.0         0.570216      0.714154        0.500595  0.865801  \n",
       "10            71.0         0.584928      0.762978        0.489868  0.968964  \n",
       "11            76.0         0.621044      0.823582        0.593179  0.967019  \n",
       "12            73.0         0.596645      0.740259        0.547727  0.884510  \n",
       "13            46.0         0.586142      0.761853        0.490671  0.946121  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_detect_train_df = pd.DataFrame([i['activity_labels'] for i in all_x_detect_train])\n",
    "for prefix in all_x_detect_test.keys():\n",
    "    x_detect_test = pd.DataFrame()\n",
    "    test_df_prefix =[]\n",
    "    for i in all_x_detect_test[prefix]:\n",
    "        s = i['activity_labels'].tolist()\n",
    "        while len(s) < len(original_detect_train_df.columns.values):\n",
    "            s.append(None)\n",
    "        test_df_prefix.append(s)\n",
    "        \n",
    "    test_df_prefix = pd.DataFrame(test_df_prefix)\n",
    "    x_detect_test = detect_encoder_features.transform(test_df_prefix)\n",
    "    x_detect_test = pd.DataFrame(x_detect_test)\n",
    "    x_detect_test.columns = ['a%s'%(i) for i in x_detect_test.columns.values]\n",
    "\n",
    "    target_label = detect_target_encoder.transform(pd.DataFrame([i['target_labels'] for i in all_x_detect_test[prefix]]))\n",
    "    target_label = pd.DataFrame(target_label)\n",
    "    x_detect_test = pd.concat([x_detect_test, target_label], axis=1)\n",
    "    for pos, i in enumerate(all_x_detect_test[prefix]):\n",
    "        while len(i['probability']) < max_prob_length:\n",
    "            all_x_detect_test[prefix][pos]['probability'].append(0)\n",
    "\n",
    "    x_detect_test = pd.concat([x_detect_test, pd.DataFrame([i['probability'] for i in all_x_detect_test[prefix]], \n",
    "             columns = ['p%s'%(i) for i in range(len(all_x_detect_train[0]['probability']))])], axis=1)\n",
    "#     x_detect_test = pd.concat([x_detect_test, pd.DataFrame([i['probability'] for i in all_x_detect_test[2]], \n",
    "#                            columns = ['p%s'%(i) for i in range(len(all_x_detect_train[0]['probability']))])], axis=1)\n",
    "    x_detect_test = pd.concat([x_detect_test, pd.DataFrame([i['ce_loss'] for i in all_x_detect_test[prefix]], columns=['ce_loss'])], axis=1)\n",
    "    x_detect_test.columns = x_detect_test.columns.astype(str)\n",
    "    print(x_detect_test.shape)\n",
    "\n",
    "    predicted_anomaly = anom_clf.predict(x_detect_test)\n",
    "    gt_anomaly_test = all_y_detect_test[prefix]\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 5: Evaluate the Anomaly Detection\n",
    "    # ----------------------------\n",
    "    print(\"\\n--- Anomaly Detection (Dynamic Threshold) Classification Report for prefix %s ---\" % prefix)\n",
    "    classification = classification_report(gt_anomaly_test, predicted_anomaly, output_dict=True)\n",
    "    f1 = classification.get('1', {}).get('f1-score', 0)\n",
    "    support = classification.get('1', {}).get('support', 0)\n",
    "    classification_result[prefix] = classification\n",
    "    print(f\"Classification Report: F1-score = {f1}, Support = {support}\")\n",
    "    classification_result[prefix]['ROC AUC'] = roc_auc_score(gt_anomaly_test, anom_clf.predict_proba(x_detect_test)[:,1])\n",
    "revised_cls_result = {}\n",
    "for i in classification_result.keys():\n",
    "    revised_cls_result[i] = dict()\n",
    "    revised_cls_result[i]['Normal precision'] =classification_result[i]['0']['precision']\n",
    "    revised_cls_result[i]['Normal recall'] =classification_result[i]['0']['recall']\n",
    "    revised_cls_result[i]['Normal f1-score'] =classification_result[i]['0']['f1-score']\n",
    "    revised_cls_result[i]['Normal support'] =classification_result[i]['0']['support']\n",
    "\n",
    "    revised_cls_result[i]['Anomal precision'] =classification_result[i]['1']['precision']\n",
    "    revised_cls_result[i]['Anomal recall'] =classification_result[i]['1']['recall']\n",
    "    revised_cls_result[i]['Anomal f1-score'] =classification_result[i]['1']['f1-score']\n",
    "    revised_cls_result[i]['Anomal support'] =classification_result[i]['1']['support']    \n",
    "\n",
    "    revised_cls_result[i]['Macro precision'] =classification_result[i]['macro avg']['precision']   \n",
    "    revised_cls_result[i]['Macro recall'] =classification_result[i]['macro avg']['recall']   \n",
    "    revised_cls_result[i]['Macro f1-score'] =classification_result[i]['macro avg']['f1-score']   \n",
    "    revised_cls_result[i]['ROC AUC'] =classification_result[i]['ROC AUC']   \n",
    "\n",
    "\n",
    "result_df = pd.DataFrame.from_dict(revised_cls_result).T\n",
    "result_df.index = result_df.index.set_names(['Prefix length'])\n",
    "result_df = result_df.reset_index(drop=False)\n",
    "# result_file_title = '../result/%s_cross_entropy_%s_anomal_thr_result.csv'%(dataset, anomaly_thr_method)\n",
    "# print(result_file_title)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "038b41ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8859398425787106\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.15      , 0.85      ],\n",
       "       [0.43166667, 0.56833333],\n",
       "       [0.473125  , 0.526875  ],\n",
       "       ...,\n",
       "       [0.755     , 0.245     ],\n",
       "       [0.53928571, 0.46071429],\n",
       "       [0.625     , 0.375     ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(roc_auc_score(gt_anomaly_test, anom_clf.predict_proba(x_detect_test)[:,1]))\n",
    "print(gt_anomaly_test)\n",
    "anom_clf.predict_proba(x_detect_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
